{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöó Entrenamiento YOLOv8 para Detecci√≥n de Partes de Veh√≠culos (TFM)\n",
    "\n",
    "## Objetivo del Experimento\n",
    "Entrenar un modelo YOLOv8 optimizado para detectar **15 clases espec√≠ficas** de partes de veh√≠culos usando un dataset balanceado y hiperpar√°metros optimizados mediante tuning autom√°tico.\n",
    "\n",
    "## Metodolog√≠a Cient√≠fica\n",
    "1. **Preparaci√≥n de datos**: Dataset balanceado con oversampling y augmentaci√≥n\n",
    "2. **Optimizaci√≥n**: Hiperpar√°metros previamente optimizados con `model.tune()`\n",
    "3. **Entrenamiento**: YOLOv8m con configuraci√≥n robusta y early stopping\n",
    "4. **Evaluaci√≥n**: M√©tricas completas en conjunto de test independiente\n",
    "5. **An√°lisis**: Convergencia, distribuci√≥n por clase y casos de fallo\n",
    "\n",
    "## Configuraci√≥n del Experimento\n",
    "- **Arquitectura**: YOLOv8 Medium (22.5M par√°metros)\n",
    "- **Dataset**: 15 clases balanceadas de partes vehiculares\n",
    "- **Hiperpar√°metros**: Optimizados mediante b√∫squeda autom√°tica\n",
    "- **Hardware**: Google Colab GPU (Tesla T4/V100)\n",
    "- **Reproducibilidad**: Semilla fija, logs completos\n",
    "\n",
    "## M√©tricas Objetivo\n",
    "- **mAP@0.5**: >0.75 (objetivo principal)\n",
    "- **mAP@0.5:0.95**: >0.45 (evaluaci√≥n estricta)\n",
    "- **Balance por clase**: CV < 0.3 (coeficiente de variaci√≥n)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paso 0: Instalaci√≥n de dependencias ---\n",
    "print(\"üîß Instalando dependencias necesarias...\")\n",
    "\n",
    "# Instalar ultralytics (YOLOv8)\n",
    "!pip install ultralytics\n",
    "\n",
    "# Instalar otras dependencias que puedan faltar\n",
    "!pip install seaborn\n",
    "\n",
    "# Verificar instalaciones\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def check_package(package_name):\n",
    "    try:\n",
    "        __import__(package_name)\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "packages_to_check = [\n",
    "    ('ultralytics', 'ultralytics'),\n",
    "    ('cv2', 'opencv-python'),\n",
    "    ('seaborn', 'seaborn'),\n",
    "    ('matplotlib', 'matplotlib'),\n",
    "    ('pandas', 'pandas'),\n",
    "    ('numpy', 'numpy'),\n",
    "    ('yaml', 'PyYAML')\n",
    "]\n",
    "\n",
    "print(\"\\nüì¶ Verificando dependencias:\")\n",
    "for package, pip_name in packages_to_check:\n",
    "    if check_package(package):\n",
    "        print(f\"‚úÖ {package} instalado correctamente\")\n",
    "    else:\n",
    "        print(f\"‚ùå {package} no encontrado, instalando...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name])\n",
    "\n",
    "print(\"\\nüéâ Todas las dependencias est√°n listas!\")\n",
    "print(\"üîÑ Ahora puedes ejecutar las siguientes celdas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paso 1: Configuraci√≥n del entorno ---\n",
    "import os\n",
    "import yaml\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ultralytics import YOLO\n",
    "from google.colab import drive, files\n",
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"üöÄ Fecha de ejecuci√≥n: {datetime.datetime.now()}\")\n",
    "\n",
    "# Verificar versi√≥n de ultralytics de forma m√°s robusta\n",
    "try:\n",
    "    import ultralytics\n",
    "    version = getattr(ultralytics, '__version__', 'unknown')\n",
    "    print(f\"üìä Versi√≥n de Ultralytics: {version}\")\n",
    "except Exception as e:\n",
    "    print(f\"üìä Ultralytics instalado (versi√≥n no detectada: {e})\")\n",
    "\n",
    "print(\"üìÅ Montando Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Directorios de trabajo\n",
    "DRIVE_PROJECT_PATH = '/content/drive/MyDrive/TFM_Proyecto_Modelos'\n",
    "RESULTS_PATH = os.path.join(DRIVE_PROJECT_PATH, 'resultados_finales')\n",
    "os.makedirs(DRIVE_PROJECT_PATH, exist_ok=True)\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Directorio de trabajo: {DRIVE_PROJECT_PATH}\")\n",
    "print(f\"‚úÖ Directorio de resultados: {RESULTS_PATH}\")\n",
    "\n",
    "# Verificar GPU disponible\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"üöÄ GPU detectada: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "    \n",
    "    # Informaci√≥n adicional de GPU\n",
    "    print(f\"üîß CUDA disponible: {torch.version.cuda}\")\n",
    "    print(f\"üîß Dispositivos disponibles: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se detect√≥ GPU. El entrenamiento ser√° m√°s lento.\")\n",
    "\n",
    "# Verificar espacio disponible en disco\n",
    "import shutil\n",
    "total, used, free = shutil.disk_usage('/content/')\n",
    "print(f\"üíæ Espacio disponible: {free // (1024**3)} GB de {total // (1024**3)} GB\")\n",
    "\n",
    "# Establecer semilla para reproducibilidad\n",
    "import random\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "print(f\"üé≤ Semilla establecida: {SEED} (para reproducibilidad)\")\n",
    "\n",
    "print(\"\\n‚úÖ Configuraci√≥n del entorno completada correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ACCI√ìN REQUERIDA:**\n",
    "Ejecuta esta celda para subir:\n",
    "1. **Dataset balanceado** (archivo ZIP)\n",
    "2. **hyp_tuned.yaml** (hiperpar√°metros optimizados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paso 2: Subir archivos necesarios ---\n",
    "print(\"üì§ Sube los archivos necesarios:\")\n",
    "print(\"1. Dataset balanceado (archivo .zip)\")\n",
    "print(\"2. Archivo hyp_tuned.yaml con hiperpar√°metros optimizados\")\n",
    "print(\"\\n‚è≥ Esperando archivos...\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Identificar archivos subidos\n",
    "dataset_zip = None\n",
    "hyp_file = None\n",
    "\n",
    "for filename in uploaded.keys():\n",
    "    if filename.endswith('.zip'):\n",
    "        dataset_zip = filename\n",
    "        file_size = len(uploaded[filename]) / (1024 * 1024)\n",
    "        print(f\"‚úÖ Dataset ZIP: {dataset_zip} ({file_size:.1f} MB)\")\n",
    "    elif 'hyp' in filename.lower() and filename.endswith('.yaml'):\n",
    "        hyp_file = filename\n",
    "        print(f\"‚úÖ Hiperpar√°metros: {hyp_file}\")\n",
    "\n",
    "# Validaciones\n",
    "if not dataset_zip:\n",
    "    raise FileNotFoundError(\"‚ùå No se encontr√≥ archivo ZIP del dataset\")\n",
    "if not hyp_file:\n",
    "    raise FileNotFoundError(\"‚ùå No se encontr√≥ archivo de hiperpar√°metros (.yaml)\")\n",
    "\n",
    "print(\"\\nüéØ Todos los archivos subidos correctamente.\")\n",
    "\n",
    "# Guardar informaci√≥n de archivos subidos\n",
    "upload_info = {\n",
    "    'timestamp': datetime.datetime.now().isoformat(),\n",
    "    'dataset_file': dataset_zip,\n",
    "    'dataset_size_mb': file_size,\n",
    "    'hyperparams_file': hyp_file\n",
    "}\n",
    "\n",
    "with open(os.path.join(RESULTS_PATH, 'upload_info.json'), 'w') as f:\n",
    "    json.dump(upload_info, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paso 3: Descomprimir y validar dataset ---\n",
    "print(f\"üì¶ Descomprimiendo {dataset_zip}...\")\n",
    "\n",
    "# Descomprimir dataset\n",
    "with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n",
    "    zip_ref.extractall('/content/')\n",
    "print(\"‚úÖ Dataset descomprimido.\")\n",
    "\n",
    "# Buscar archivo data.yaml autom√°ticamente\n",
    "data_yaml_path = None\n",
    "dataset_root = None\n",
    "\n",
    "for root, dirs, files in os.walk('/content/'):\n",
    "    if 'data.yaml' in files:\n",
    "        data_yaml_path = os.path.join(root, 'data.yaml')\n",
    "        dataset_root = root\n",
    "        break\n",
    "\n",
    "if not data_yaml_path:\n",
    "    raise FileNotFoundError(\"‚ùå No se encontr√≥ data.yaml en el dataset descomprimido.\")\n",
    "\n",
    "print(f\"üìã Dataset root: {dataset_root}\")\n",
    "print(f\"üìã data.yaml encontrado: {data_yaml_path}\")\n",
    "\n",
    "# Leer y validar data.yaml\n",
    "with open(data_yaml_path, 'r') as file:\n",
    "    data_yaml_content = yaml.safe_load(file)\n",
    "    class_names = data_yaml_content.get('names', [])\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "print(f\"\\nüéØ Dataset Information:\")\n",
    "print(f\"  üìä N√∫mero de clases: {num_classes}\")\n",
    "print(f\"  üè∑Ô∏è Clases: {class_names}\")\n",
    "\n",
    "# Verificar estructura de directorios y contar archivos\n",
    "splits = ['train', 'valid', 'test']\n",
    "dataset_stats = {}\n",
    "\n",
    "for split in splits:\n",
    "    img_dir = os.path.join(dataset_root, split, 'images')\n",
    "    lbl_dir = os.path.join(dataset_root, split, 'labels')\n",
    "    \n",
    "    if os.path.exists(img_dir) and os.path.exists(lbl_dir):\n",
    "        img_count = len([f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        lbl_count = len([f for f in os.listdir(lbl_dir) if f.endswith('.txt')])\n",
    "        \n",
    "        dataset_stats[split] = {\n",
    "            'images': img_count,\n",
    "            'labels': lbl_count,\n",
    "            'match': img_count == lbl_count\n",
    "        }\n",
    "        \n",
    "        status = \"‚úÖ\" if img_count == lbl_count else \"‚ö†Ô∏è\"\n",
    "        print(f\"  üìÅ {split}: {img_count} im√°genes, {lbl_count} etiquetas {status}\")\n",
    "    else:\n",
    "        dataset_stats[split] = {'images': 0, 'labels': 0, 'match': False}\n",
    "        print(f\"  ‚ùå {split}: Directorio no encontrado\")\n",
    "\n",
    "# Guardar estad√≠sticas del dataset\n",
    "dataset_info = {\n",
    "    'num_classes': num_classes,\n",
    "    'class_names': class_names,\n",
    "    'dataset_root': dataset_root,\n",
    "    'data_yaml_path': data_yaml_path,\n",
    "    'splits_stats': dataset_stats,\n",
    "    'validation_timestamp': datetime.datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(os.path.join(RESULTS_PATH, 'dataset_info.json'), 'w') as f:\n",
    "    json.dump(dataset_info, f, indent=2)\n",
    "\n",
    "print(f\"\\nüìä Informaci√≥n del dataset guardada en: {RESULTS_PATH}/dataset_info.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paso 4: Configurar hiperpar√°metros optimizados ---\n",
    "HYP_YAML_PATH = f'/content/{hyp_file}'\n",
    "\n",
    "# Verificar y leer hiperpar√°metros\n",
    "if not os.path.exists(HYP_YAML_PATH):\n",
    "    raise FileNotFoundError(f\"‚ùå No se encontr√≥ {HYP_YAML_PATH}\")\n",
    "\n",
    "with open(HYP_YAML_PATH, 'r') as f:\n",
    "    hyp_content = yaml.safe_load(f)\n",
    "\n",
    "print(\"‚öôÔ∏è Hiperpar√°metros optimizados cargados:\")\n",
    "print(\"=\"*50)\n",
    "for category, params in {\n",
    "    'Optimizaci√≥n': ['lr0', 'lrf', 'momentum', 'weight_decay'],\n",
    "    'Warmup': ['warmup_epochs', 'warmup_momentum'],\n",
    "    'Loss': ['box', 'cls', 'dfl'],\n",
    "    'Augmentaci√≥n HSV': ['hsv_h', 'hsv_s', 'hsv_v'],\n",
    "    'Augmentaci√≥n Geom√©trica': ['degrees', 'translate', 'scale', 'fliplr'],\n",
    "    'Augmentaci√≥n Avanzada': ['mosaic', 'mixup', 'copy_paste']\n",
    "}.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for param in params:\n",
    "        if param in hyp_content:\n",
    "            print(f\"  {param}: {hyp_content[param]}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Archivo de hiperpar√°metros listo: {HYP_YAML_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paso 5: An√°lisis del dataset balanceado ---\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_dataset_distribution(labels_dir, class_names):\n",
    "    \"\"\"Analiza la distribuci√≥n de clases en un directorio de etiquetas.\"\"\"\n",
    "    class_counts = Counter()\n",
    "    total_objects = 0\n",
    "    \n",
    "    if not os.path.exists(labels_dir):\n",
    "        return class_counts, total_objects\n",
    "    \n",
    "    for label_file in os.listdir(labels_dir):\n",
    "        if label_file.endswith('.txt'):\n",
    "            label_path = os.path.join(labels_dir, label_file)\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        class_id = int(line.strip().split()[0])\n",
    "                        class_counts[class_id] += 1\n",
    "                        total_objects += 1\n",
    "                    except (ValueError, IndexError):\n",
    "                        continue\n",
    "    \n",
    "    return class_counts, total_objects\n",
    "\n",
    "# Analizar distribuci√≥n por split\n",
    "print(\"üìä An√°lisis de distribuci√≥n del dataset balanceado:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "distribution_data = []\n",
    "\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    labels_dir = os.path.join(dataset_root, split, 'labels')\n",
    "    class_counts, total_objects = analyze_dataset_distribution(labels_dir, class_names)\n",
    "    \n",
    "    print(f\"\\n{split.upper()} SET:\")\n",
    "    print(f\"  Total objetos: {total_objects}\")\n",
    "    \n",
    "    for class_id in range(len(class_names)):\n",
    "        count = class_counts.get(class_id, 0)\n",
    "        percentage = (count / total_objects * 100) if total_objects > 0 else 0\n",
    "        print(f\"  {class_names[class_id]}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        distribution_data.append({\n",
    "            'Split': split,\n",
    "            'Clase': class_names[class_id],\n",
    "            'Clase_ID': class_id,\n",
    "            'Cantidad': count,\n",
    "            'Porcentaje': percentage\n",
    "        })\n",
    "\n",
    "# Crear DataFrame para an√°lisis\n",
    "df_distribution = pd.DataFrame(distribution_data)\n",
    "\n",
    "# Visualizar distribuci√≥n\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Subplot 1: Distribuci√≥n por split\n",
    "plt.subplot(2, 2, 1)\n",
    "train_data = df_distribution[df_distribution['Split'] == 'train']\n",
    "plt.bar(train_data['Clase'], train_data['Cantidad'])\n",
    "plt.title('Distribuci√≥n de Clases - Train Set')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Cantidad de Instancias')\n",
    "\n",
    "# Subplot 2: Comparaci√≥n entre splits\n",
    "plt.subplot(2, 2, 2)\n",
    "pivot_data = df_distribution.pivot(index='Clase', columns='Split', values='Cantidad').fillna(0)\n",
    "pivot_data.plot(kind='bar', ax=plt.gca())\n",
    "plt.title('Comparaci√≥n entre Train/Valid/Test')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Split')\n",
    "\n",
    "# Subplot 3: Balance del train set\n",
    "plt.subplot(2, 2, 3)\n",
    "train_counts = train_data['Cantidad'].values\n",
    "plt.hist(train_counts, bins=10, alpha=0.7, edgecolor='black')\n",
    "plt.title('Histograma de Balance - Train Set')\n",
    "plt.xlabel('Cantidad de Instancias por Clase')\n",
    "plt.ylabel('N√∫mero de Clases')\n",
    "plt.axvline(np.mean(train_counts), color='red', linestyle='--', label=f'Media: {np.mean(train_counts):.0f}')\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 4: Estad√≠sticas de balance\n",
    "plt.subplot(2, 2, 4)\n",
    "balance_stats = {\n",
    "    'Media': np.mean(train_counts),\n",
    "    'Mediana': np.median(train_counts),\n",
    "    'Desv. Est√°ndar': np.std(train_counts),\n",
    "    'Min': np.min(train_counts),\n",
    "    'Max': np.max(train_counts),\n",
    "    'Coef. Variaci√≥n': np.std(train_counts) / np.mean(train_counts)\n",
    "}\n",
    "\n",
    "stats_text = \"\\n\".join([f\"{k}: {v:.2f}\" for k, v in balance_stats.items()])\n",
    "plt.text(0.1, 0.5, stats_text, transform=plt.gca().transAxes, fontsize=12,\n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
    "plt.title('Estad√≠sticas de Balance')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_PATH, 'dataset_distribution_analysis.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä An√°lisis guardado en: {RESULTS_PATH}/dataset_distribution_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paso 6: Entrenamiento YOLOv8 optimizado ---\n",
    "print(\"üöÄ Iniciando entrenamiento con configuraci√≥n optimizada para TFM...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Configuraci√≥n de entrenamiento optimizada\n",
    "EXPERIMENT_NAME = 'Parts_Detector_TFM_Final'\n",
    "EPOCHS = 150  # Aumentado para mejor convergencia\n",
    "PATIENCE = 50  # Mayor paciencia\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = 640\n",
    "\n",
    "print(f\"üìã Configuraci√≥n de entrenamiento:\")\n",
    "print(f\"  üéØ Experimento: {EXPERIMENT_NAME}\")\n",
    "print(f\"  üîÑ √âpocas m√°ximas: {EPOCHS}\")\n",
    "print(f\"  ‚è≥ Paciencia: {PATIENCE}\")\n",
    "print(f\"  üì¶ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  üñºÔ∏è Tama√±o imagen: {IMG_SIZE}\")\n",
    "print(f\"  ‚öôÔ∏è Hiperpar√°metros: Optimizados con tune()\")\n",
    "\n",
    "# Cargar modelo base\n",
    "model = YOLO('yolov8m.pt')\n",
    "print(f\"‚úÖ Modelo YOLOv8m cargado\")\n",
    "\n",
    "# Iniciar entrenamiento\n",
    "start_time = datetime.datetime.now()\n",
    "print(f\"\\nüèÅ Iniciando entrenamiento: {start_time}\")\n",
    "\n",
    "results = model.train(\n",
    "    data=data_yaml_path,\n",
    "    hyp=HYP_YAML_PATH,  # Usar hiperpar√°metros optimizados\n",
    "    epochs=EPOCHS,\n",
    "    patience=PATIENCE,\n",
    "    imgsz=IMG_SIZE,\n",
    "    batch=BATCH_SIZE,\n",
    "    optimizer='AdamW',  # Optimizador robusto\n",
    "    project=DRIVE_PROJECT_PATH,\n",
    "    name=EXPERIMENT_NAME,\n",
    "    exist_ok=True,\n",
    "    save=True,\n",
    "    save_period=25,     # Guardar cada 25 √©pocas\n",
    "    plots=True,         # Generar todos los plots\n",
    "    verbose=True,\n",
    "    workers=2,          # Optimizado para Colab\n",
    "    seed=42             # Reproducibilidad\n",
    ")\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "training_duration = end_time - start_time\n",
    "\n",
    "print(f\"\\nüéâ Entrenamiento completado!\")\n",
    "print(f\"‚è±Ô∏è Duraci√≥n total: {training_duration}\")\n",
    "print(f\"üìÅ Resultados guardados en: {DRIVE_PROJECT_PATH}/{EXPERIMENT_NAME}\")\n",
    "\n",
    "# Guardar informaci√≥n del entrenamiento\n",
    "training_info = {\n",
    "    'experiment_name': EXPERIMENT_NAME,\n",
    "    'start_time': start_time.isoformat(),\n",
    "    'end_time': end_time.isoformat(),\n",
    "    'duration_seconds': training_duration.total_seconds(),\n",
    "    'epochs': EPOCHS,\n",
    "    'patience': PATIENCE,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'img_size': IMG_SIZE,\n",
    "    'num_classes': num_classes,\n",
    "    'class_names': class_names\n",
    "}\n",
    "\n",
    "with open(os.path.join(RESULTS_PATH, 'training_info.json'), 'w') as f:\n",
    "    json.dump(training_info, f, indent=2)\n",
    "\n",
    "print(f\"üìä Informaci√≥n del entrenamiento guardada en: {RESULTS_PATH}/training_info.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paso 7: An√°lisis de convergencia y curvas de entrenamiento ---\n",
    "experiment_dir = os.path.join(DRIVE_PROJECT_PATH, EXPERIMENT_NAME)\n",
    "results_csv_path = os.path.join(experiment_dir, 'results.csv')\n",
    "\n",
    "if os.path.exists(results_csv_path):\n",
    "    print(\"üìà Analizando curvas de entrenamiento...\")\n",
    "    \n",
    "    # Cargar resultados\n",
    "    df_results = pd.read_csv(results_csv_path)\n",
    "    df_results = df_results.fillna(0)  # Rellenar NaN con 0\n",
    "    \n",
    "    # Crear visualizaci√≥n de convergencia\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Loss curves\n",
    "    axes[0,0].plot(df_results['epoch'], df_results['train/box_loss'], label='Train Box Loss', color='blue')\n",
    "    axes[0,0].plot(df_results['epoch'], df_results['val/box_loss'], label='Val Box Loss', color='red')\n",
    "    axes[0,0].set_title('Box Loss Evolution')\n",
    "    axes[0,0].set_xlabel('Epoch')\n",
    "    axes[0,0].set_ylabel('Loss')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Classification loss\n",
    "    axes[0,1].plot(df_results['epoch'], df_results['train/cls_loss'], label='Train Cls Loss', color='green')\n",
    "    axes[0,1].plot(df_results['epoch'], df_results['val/cls_loss'], label='Val Cls Loss', color='orange')\n",
    "    axes[0,1].set_title('Classification Loss Evolution')\n",
    "    axes[0,1].set_xlabel('Epoch')\n",
    "    axes[0,1].set_ylabel('Loss')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. mAP metrics\n",
    "    axes[0,2].plot(df_results['epoch'], df_results['metrics/mAP50(B)'], label='mAP@0.5', color='purple')\n",
    "    axes[0,2].plot(df_results['epoch'], df_results['metrics/mAP50-95(B)'], label='mAP@0.5:0.95', color='brown')\n",
    "    axes[0,2].set_title('mAP Evolution')\n",
    "    axes[0,2].set_xlabel('Epoch')\n",
    "    axes[0,2].set_ylabel('mAP')\n",
    "    axes[0,2].legend()\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Precision and Recall\n",
    "    axes[1,0].plot(df_results['epoch'], df_results['metrics/precision(B)'], label='Precision', color='cyan')\n",
    "    axes[1,0].plot(df_results['epoch'], df_results['metrics/recall(B)'], label='Recall', color='magenta')\n",
    "    axes[1,0].set_title('Precision & Recall Evolution')\n",
    "    axes[1,0].set_xlabel('Epoch')\n",
    "    axes[1,0].set_ylabel('Score')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Learning rate\n",
    "    if 'lr/pg0' in df_results.columns:\n",
    "        axes[1,1].plot(df_results['epoch'], df_results['lr/pg0'], label='Learning Rate', color='red')\n",
    "        axes[1,1].set_title('Learning Rate Schedule')\n",
    "        axes[1,1].set_xlabel('Epoch')\n",
    "        axes[1,1].set_ylabel('Learning Rate')\n",
    "        axes[1,1].legend()\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1,1].text(0.5, 0.5, 'Learning Rate\\ndata not available', \n",
    "                      ha='center', va='center', transform=axes[1,1].transAxes)\n",
    "        axes[1,1].set_title('Learning Rate Schedule')\n",
    "    \n",
    "    # 6. An√°lisis de convergencia\n",
    "    last_10_epochs = df_results.tail(10)\n",
    "    map50_trend = last_10_epochs['metrics/mAP50(B)'].is_monotonic_increasing\n",
    "    \n",
    "    convergence_text = f\"An√°lisis de Convergencia\\n\\n\"\n",
    "    convergence_text += f\"√âpocas totales: {len(df_results)}\\n\"\n",
    "    convergence_text += f\"Mejor mAP@0.5: {df_results['metrics/mAP50(B)'].max():.4f}\\n\"\n",
    "    convergence_text += f\"√âpoca del mejor mAP: {df_results['metrics/mAP50(B)'].idxmax() + 1}\\n\\n\"\n",
    "    \n",
    "    if map50_trend:\n",
    "        convergence_text += \"üü° Modelo a√∫n mejorando\\nen √∫ltimas √©pocas\"\n",
    "    else:\n",
    "        convergence_text += \"üü¢ Modelo convergido\\ncorrectamente\"\n",
    "    \n",
    "    axes[1,2].text(0.1, 0.5, convergence_text, transform=axes[1,2].transAxes, \n",
    "                   fontsize=11, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\"))\n",
    "    axes[1,2].set_title('Convergence Analysis')\n",
    "    axes[1,2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_PATH, 'training_curves_analysis.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Guardar estad√≠sticas de entrenamiento\n",
    "    training_stats = {\n",
    "        'total_epochs': len(df_results),\n",
    "        'best_map50': float(df_results['metrics/mAP50(B)'].max()),\n",
    "        'best_map50_95': float(df_results['metrics/mAP50-95(B)'].max()),\n",
    "        'best_precision': float(df_results['metrics/precision(B)'].max()),\n",
    "        'best_recall': float(df_results['metrics/recall(B)'].max()),\n",
    "        'final_box_loss_train': float(df_results['train/box_loss'].iloc[-1]),\n",
    "        'final_box_loss_val': float(df_results['val/box_loss'].iloc[-1]),\n",
    "        'converged': not map50_trend\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(RESULTS_PATH, 'training_stats.json'), 'w') as f:\n",
    "        json.dump(training_stats, f, indent=2)\n",
    "    \n",
    "    print(f\"üìä An√°lisis de convergencia guardado en: {RESULTS_PATH}/training_curves_analysis.png\")\n",
    "    print(f\"üìà Estad√≠sticas guardadas en: {RESULTS_PATH}/training_stats.json\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se encontr√≥ archivo results.csv para an√°lisis de convergencia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paso 8: Evaluaci√≥n completa del modelo ---\n",
    "best_model_path = os.path.join(experiment_dir, 'weights/best.pt')\n",
    "\n",
    "if not os.path.exists(best_model_path):\n",
    "    raise FileNotFoundError(f\"‚ùå No se encontr√≥ el modelo entrenado en: {best_model_path}\")\n",
    "\n",
    "# Cargar el mejor modelo\n",
    "model = YOLO(best_model_path)\n",
    "print(f\"‚úÖ Modelo cargado desde: {best_model_path}\")\n",
    "\n",
    "# Evaluaci√≥n en conjunto de test\n",
    "print(\"\\nüìä Evaluando modelo en conjunto de test...\")\n",
    "test_metrics = model.val(data=data_yaml_path, split='test', save_json=True)\n",
    "\n",
    "# Extraer m√©tricas principales\n",
    "if hasattr(test_metrics, 'box'):\n",
    "    map50 = test_metrics.box.map50\n",
    "    map50_95 = test_metrics.box.map\n",
    "    precision = test_metrics.box.mp\n",
    "    recall = test_metrics.box.mr\n",
    "    \n",
    "    print(f\"\\nüéØ M√©tricas finales en test set:\")\n",
    "    print(f\"  üìà mAP@0.5: {map50:.4f}\")\n",
    "    print(f\"  üìà mAP@0.5:0.95: {map50_95:.4f}\")\n",
    "    print(f\"  üéØ Precisi√≥n: {precision:.4f}\")\n",
    "    print(f\"  üîç Recall: {recall:.4f}\")\n",
    "    print(f\"  ‚öñÔ∏è F1-Score: {2 * (precision * recall) / (precision + recall):.4f}\")\n",
    "    \n",
    "    # M√©tricas por clase si est√°n disponibles\n",
    "    if hasattr(test_metrics.box, 'ap_class_index') and hasattr(test_metrics.box, 'ap'):\n",
    "        print(f\"\\nüìä M√©tricas por clase (mAP@0.5):\")\n",
    "        class_metrics_data = []\n",
    "        \n",
    "        for i, class_name in enumerate(class_names):\n",
    "            if i < len(test_metrics.box.ap):\n",
    "                ap_score = test_metrics.box.ap[i]\n",
    "                print(f\"  {class_name}: {ap_score:.4f}\")\n",
    "                class_metrics_data.append({\n",
    "                    'Clase': class_name,\n",
    "                    'Clase_ID': i,\n",
    "                    'AP@0.5': ap_score\n",
    "                })\n",
    "        \n",
    "        # Crear DataFrame y visualizaci√≥n de m√©tricas por clase\n",
    "        df_class_metrics = pd.DataFrame(class_metrics_data)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        bars = plt.bar(df_class_metrics['Clase'], df_class_metrics['AP@0.5'])\n",
    "        plt.title('Average Precision (AP@0.5) por Clase')\n",
    "        plt.xlabel('Clases')\n",
    "        plt.ylabel('AP@0.5')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        # Colorear barras seg√∫n rendimiento\n",
    "        for bar, ap in zip(bars, df_class_metrics['AP@0.5']):\n",
    "            if ap >= 0.8:\n",
    "                bar.set_color('green')\n",
    "            elif ap >= 0.6:\n",
    "                bar.set_color('orange')\n",
    "            else:\n",
    "                bar.set_color('red')\n",
    "        \n",
    "        # A√±adir l√≠nea de promedio\n",
    "        plt.axhline(y=map50, color='blue', linestyle='--', \n",
    "                   label=f'mAP promedio: {map50:.3f}')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(RESULTS_PATH, 'class_performance_analysis.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Guardar m√©tricas por clase\n",
    "        df_class_metrics.to_csv(os.path.join(RESULTS_PATH, 'class_metrics.csv'), index=False)\n",
    "        print(f\"üìä M√©tricas por clase guardadas en: {RESULTS_PATH}/class_metrics.csv\")\n",
    "    \n",
    "    # Guardar m√©tricas finales\n",
    "    final_metrics = {\n",
    "        'map50': float(map50),\n",
    "        'map50_95': float(map50_95),\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'f1_score': float(2 * (precision * recall) / (precision + recall)),\n",
    "        'num_classes': num_classes,\n",
    "        'test_evaluation_date': datetime.datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(RESULTS_PATH, 'final_metrics.json'), 'w') as f:\n",
    "        json.dump(final_metrics, f, indent=2)\n",
    "    \n",
    "    print(f\"üìä M√©tricas finales guardadas en: {RESULTS_PATH}/final_metrics.json\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se pudieron extraer m√©tricas del modelo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paso 9: Visualizaci√≥n de resultados del entrenamiento ---\n",
    "print(\"üñºÔ∏è Generando visualizaciones de resultados...\")\n",
    "\n",
    "# Copiar im√°genes importantes a carpeta de resultados\n",
    "important_plots = [\n",
    "    ('confusion_matrix.png', 'Matriz de Confusi√≥n'),\n",
    "    ('PR_curve.png', 'Curva Precisi√≥n-Recall'),\n",
    "    ('F1_curve.png', 'Curva F1'),\n",
    "    ('results.png', 'Resumen de M√©tricas'),\n",
    "    ('val_batch0_labels.jpg', 'Ejemplos de Validaci√≥n - Labels'),\n",
    "    ('val_batch0_pred.jpg', 'Ejemplos de Validaci√≥n - Predicciones')\n",
    "]\n",
    "\n",
    "available_plots = []\n",
    "for plot_file, plot_name in important_plots:\n",
    "    src_path = os.path.join(experiment_dir, plot_file)\n",
    "    if os.path.exists(src_path):\n",
    "        dst_path = os.path.join(RESULTS_PATH, plot_file)\n",
    "        import shutil\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "        available_plots.append((dst_path, plot_name))\n",
    "        print(f\"‚úÖ {plot_name} copiado a resultados\")\n",
    "\n",
    "# Mostrar visualizaciones\n",
    "print(\"\\nüìä Visualizando resultados principales:\")\n",
    "\n",
    "for plot_path, plot_name in available_plots[:4]:  # Mostrar solo los primeros 4\n",
    "    if os.path.exists(plot_path):\n",
    "        img = plt.imread(plot_path)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(img)\n",
    "        plt.title(f'{plot_name} - Modelo Final')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(f\"üìä {plot_name} mostrado\")\n",
    "\n",
    "print(f\"\\nüìÅ Todas las visualizaciones est√°n disponibles en: {RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paso 10: Ejemplos de predicci√≥n en im√°genes de test ---\n",
    "test_images_dir = os.path.join(dataset_root, 'test/images')\n",
    "\n",
    "if os.path.exists(test_images_dir):\n",
    "    test_images = [f for f in os.listdir(test_images_dir) \n",
    "                   if f.lower().endswith(('.jpg', '.jpeg', '.png'))][:5]\n",
    "    \n",
    "    if test_images:\n",
    "        print(\"üîç Generando ejemplos de predicci√≥n en im√°genes de test:\")\n",
    "        \n",
    "        prediction_results = []\n",
    "        \n",
    "        for i, img_file in enumerate(test_images):\n",
    "            img_path = os.path.join(test_images_dir, img_file)\n",
    "            \n",
    "            print(f\"\\n--- Imagen {i+1}: {img_file} ---\")\n",
    "            \n",
    "            # Realizar predicci√≥n\n",
    "            results = model.predict(img_path, conf=0.25, save=False, verbose=False)\n",
    "            \n",
    "            # Extraer informaci√≥n de detecciones\n",
    "            detections = []\n",
    "            if len(results) > 0 and results[0].boxes is not None:\n",
    "                boxes = results[0].boxes\n",
    "                for j in range(len(boxes)):\n",
    "                    class_id = int(boxes.cls[j])\n",
    "                    confidence = float(boxes.conf[j])\n",
    "                    class_name = class_names[class_id] if class_id < len(class_names) else f\"Clase_{class_id}\"\n",
    "                    \n",
    "                    detections.append({\n",
    "                        'class_name': class_name,\n",
    "                        'confidence': confidence\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"  üéØ {class_name}: {confidence:.3f}\")\n",
    "            else:\n",
    "                print(\"  ‚ùå No se detectaron objetos\")\n",
    "            \n",
    "            prediction_results.append({\n",
    "                'image': img_file,\n",
    "                'detections': detections,\n",
    "                'num_detections': len(detections)\n",
    "            })\n",
    "            \n",
    "            # Mostrar imagen con predicciones\n",
    "            if len(results) > 0:\n",
    "                # Guardar imagen con predicciones\n",
    "                annotated_img = results[0].plot()\n",
    "                save_path = os.path.join(RESULTS_PATH, f'prediction_example_{i+1}.jpg')\n",
    "                \n",
    "                import cv2\n",
    "                cv2.imwrite(save_path, annotated_img)\n",
    "                \n",
    "                # Mostrar imagen\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                plt.imshow(cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB))\n",
    "                plt.title(f'Predicci√≥n {i+1}: {img_file}')\n",
    "                plt.axis('off')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        \n",
    "        # Guardar resultados de predicciones\n",
    "        with open(os.path.join(RESULTS_PATH, 'prediction_examples.json'), 'w') as f:\n",
    "            json.dump(prediction_results, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nüìä Ejemplos de predicci√≥n guardados en: {RESULTS_PATH}/prediction_examples.json\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No se encontraron im√°genes en el directorio de test\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No se encontr√≥ directorio de test: {test_images_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paso 11: Exportaci√≥n del modelo en m√∫ltiples formatos ---\n",
    "print(\"üì¶ Exportando modelo final en m√∫ltiples formatos...\")\n",
    "\n",
    "export_results = {}\n",
    "export_formats = ['onnx', 'torchscript', 'engine']  # Formatos m√°s comunes\n",
    "\n",
    "for fmt in export_formats:\n",
    "    try:\n",
    "        print(f\"\\nüì§ Exportando a {fmt.upper()}...\")\n",
    "        exported_path = model.export(format=fmt, dynamic=True, simplify=True)\n",
    "        \n",
    "        # Obtener tama√±o del archivo\n",
    "        if os.path.exists(exported_path):\n",
    "            file_size = os.path.getsize(exported_path) / (1024 * 1024)  # MB\n",
    "            export_results[fmt] = {\n",
    "                'path': exported_path,\n",
    "                'size_mb': file_size,\n",
    "                'success': True\n",
    "            }\n",
    "            print(f\"‚úÖ {fmt.upper()}: {exported_path} ({file_size:.1f} MB)\")\n",
    "        else:\n",
    "            export_results[fmt] = {'success': False, 'error': 'File not found after export'}\n",
    "            print(f\"‚ùå {fmt.upper()}: Error - archivo no encontrado\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        export_results[fmt] = {'success': False, 'error': str(e)}\n",
    "        print(f\"‚ùå {fmt.upper()}: Error - {str(e)}\")\n",
    "\n",
    "# Informaci√≥n del modelo original\n",
    "original_model_size = os.path.getsize(best_model_path) / (1024 * 1024)\n",
    "export_results['pytorch'] = {\n",
    "    'path': best_model_path,\n",
    "    'size_mb': original_model_size,\n",
    "    'success': True\n",
    "}\n",
    "\n",
    "print(f\"\\nüìã Resumen de exportaciones:\")\n",
    "print(f\"  üîß Modelo PyTorch original: {original_model_size:.1f} MB\")\n",
    "for fmt, result in export_results.items():\n",
    "    if result['success']:\n",
    "        print(f\"  ‚úÖ {fmt.upper()}: {result['size_mb']:.1f} MB\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå {fmt.upper()}: Fallo en exportaci√≥n\")\n",
    "\n",
    "# Guardar informaci√≥n de exportaciones\n",
    "with open(os.path.join(RESULTS_PATH, 'export_results.json'), 'w') as f:\n",
    "    json.dump(export_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nüìä Informaci√≥n de exportaciones guardada en: {RESULTS_PATH}/export_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paso 12: Reporte final y resumen para TFM ---\n",
    "print(\"üìÑ Generando reporte final para TFM...\")\n",
    "\n",
    "# Recopilar toda la informaci√≥n\n",
    "final_report = {\n",
    "    'experiment_info': {\n",
    "        'name': EXPERIMENT_NAME,\n",
    "        'date': datetime.datetime.now().isoformat(),\n",
    "        'duration_hours': training_duration.total_seconds() / 3600,\n",
    "        'model_architecture': 'YOLOv8m',\n",
    "        'optimization_method': 'Hyperparameter tuning + Balanced dataset'\n",
    "    },\n",
    "    'dataset_info': {\n",
    "        'num_classes': num_classes,\n",
    "        'class_names': class_names,\n",
    "        'balancing_method': 'Oversampling with augmentation',\n",
    "        'splits': ['train', 'valid', 'test']\n",
    "    },\n",
    "    'training_config': {\n",
    "        'epochs': EPOCHS,\n",
    "        'patience': PATIENCE,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'image_size': IMG_SIZE,\n",
    "        'optimizer': 'AdamW',\n",
    "        'hyperparameters_optimized': True\n",
    "    },\n",
    "    'final_performance': {\n",
    "        'map50': float(map50) if 'map50' in locals() else None,\n",
    "        'map50_95': float(map50_95) if 'map50_95' in locals() else None,\n",
    "        'precision': float(precision) if 'precision' in locals() else None,\n",
    "        'recall': float(recall) if 'recall' in locals() else None,\n",
    "        'f1_score': float(2 * (precision * recall) / (precision + recall)) if 'precision' in locals() and 'recall' in locals() else None\n",
    "    },\n",
    "    'files_generated': {\n",
    "        'model_pytorch': best_model_path,\n",
    "        'results_directory': RESULTS_PATH,\n",
    "        'training_curves': os.path.join(RESULTS_PATH, 'training_curves_analysis.png'),\n",
    "        'class_performance': os.path.join(RESULTS_PATH, 'class_performance_analysis.png'),\n",
    "        'dataset_analysis': os.path.join(RESULTS_PATH, 'dataset_distribution_analysis.png'),\n",
    "        'confusion_matrix': os.path.join(RESULTS_PATH, 'confusion_matrix.png'),\n",
    "        'metrics_csv': os.path.join(RESULTS_PATH, 'class_metrics.csv'),\n",
    "        'final_metrics_json': os.path.join(RESULTS_PATH, 'final_metrics.json')\n",
    "    },\n",
    "    'export_formats': export_results,\n",
    "    'recommendations': [\n",
    "        \"Modelo listo para producci√≥n\",\n",
    "        \"Dataset balanceado mejora la equidad entre clases\",\n",
    "        \"Hiperpar√°metros optimizados mediante tuning autom√°tico\",\n",
    "        \"M√©tricas de evaluaci√≥n completas disponibles\",\n",
    "        \"M√∫ltiples formatos de exportaci√≥n para diferentes plataformas\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Guardar reporte final\n",
    "with open(os.path.join(RESULTS_PATH, 'REPORTE_FINAL_TFM.json'), 'w') as f:\n",
    "    json.dump(final_report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Crear resumen en texto para f√°cil lectura\n",
    "summary_text = f\"\"\"\n",
    "# REPORTE FINAL - ENTRENAMIENTO YOLOv8 PARA DETECCI√ìN DE PARTES DE VEH√çCULOS\n",
    "================================================================================\n",
    "\n",
    "## INFORMACI√ìN DEL EXPERIMENTO\n",
    "- Nombre: {EXPERIMENT_NAME}\n",
    "- Fecha: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- Duraci√≥n: {training_duration.total_seconds()/3600:.2f} horas\n",
    "- Arquitectura: YOLOv8m\n",
    "\n",
    "## CONFIGURACI√ìN DEL ENTRENAMIENTO\n",
    "- √âpocas: {EPOCHS} (con early stopping patience={PATIENCE})\n",
    "- Batch size: {BATCH_SIZE}\n",
    "- Tama√±o imagen: {IMG_SIZE}x{IMG_SIZE}\n",
    "- Optimizador: AdamW\n",
    "- Hiperpar√°metros: Optimizados con tune()\n",
    "\n",
    "## DATASET\n",
    "- Clases: {num_classes}\n",
    "- M√©todo de balanceo: Oversampling con augmentaci√≥n\n",
    "- Splits: Train/Valid/Test\n",
    "\n",
    "## RENDIMIENTO FINAL\n",
    "\"\"\"\n",
    "\n",
    "if 'map50' in locals():\n",
    "    summary_text += f\"\"\"\n",
    "- mAP@0.5: {map50:.4f}\n",
    "- mAP@0.5:0.95: {map50_95:.4f}\n",
    "- Precisi√≥n: {precision:.4f}\n",
    "- Recall: {recall:.4f}\n",
    "- F1-Score: {2 * (precision * recall) / (precision + recall):.4f}\n",
    "\"\"\"\n",
    "\n",
    "summary_text += f\"\"\"\n",
    "\n",
    "## ARCHIVOS GENERADOS\n",
    "- Modelo PyTorch: {best_model_path}\n",
    "- Directorio de resultados: {RESULTS_PATH}\n",
    "- Visualizaciones: Curvas de entrenamiento, matriz de confusi√≥n, an√°lisis por clase\n",
    "- M√©tricas: JSON y CSV con m√©tricas detalladas\n",
    "- Exportaciones: ONNX, TorchScript (si exitosas)\n",
    "\n",
    "## CONCLUSIONES\n",
    "‚úÖ Modelo entrenado exitosamente con hiperpar√°metros optimizados\n",
    "‚úÖ Dataset balanceado para mejor equidad entre clases\n",
    "‚úÖ Evaluaci√≥n completa en conjunto de test independiente\n",
    "‚úÖ M√∫ltiples formatos de exportaci√≥n para despliegue\n",
    "‚úÖ Documentaci√≥n completa para replicabilidad\n",
    "\n",
    "================================================================================\n",
    "Todos los archivos est√°n guardados en Google Drive para acceso posterior.\n",
    "\"\"\"\n",
    "\n",
    "# Guardar resumen en texto\n",
    "with open(os.path.join(RESULTS_PATH, 'RESUMEN_FINAL_TFM.txt'), 'w', encoding='utf-8') as f:\n",
    "    f.write(summary_text)\n",
    "\n",
    "print(summary_text)\n",
    "print(f\"\\nüìä Reporte final guardado en: {RESULTS_PATH}/REPORTE_FINAL_TFM.json\")\n",
    "print(f\"üìÑ Resumen guardado en: {RESULTS_PATH}/RESUMEN_FINAL_TFM.txt\")\n",
    "print(f\"\\nüéâ ¬°ENTRENAMIENTO Y EVALUACI√ìN COMPLETADOS EXITOSAMENTE!\")\n",
    "print(f\"üìÅ Todos los archivos est√°n disponibles en: {RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ ¬°Entrenamiento Completado!\n",
    "\n",
    "### Archivos generados para tu TFM:\n",
    "\n",
    "1. **Modelo entrenado**: `best.pt` (modelo PyTorch optimizado)\n",
    "2. **M√©tricas de evaluaci√≥n**: JSON y CSV con m√©tricas detalladas\n",
    "3. **Visualizaciones**:\n",
    "   - Curvas de entrenamiento y convergencia\n",
    "   - Matriz de confusi√≥n\n",
    "   - An√°lisis de rendimiento por clase\n",
    "   - Distribuci√≥n del dataset\n",
    "4. **Ejemplos de predicci√≥n**: Im√°genes con detecciones\n",
    "5. **Modelos exportados**: ONNX, TorchScript para despliegue\n",
    "6. **Reportes finales**: Documentaci√≥n completa para tu memoria\n",
    "\n",
    "### Pr√≥ximos pasos para tu TFM:\n",
    "- Utiliza las visualizaciones en tu memoria\n",
    "- Analiza las m√©tricas por clase para identificar fortalezas/debilidades\n",
    "- Usa el modelo exportado para aplicaciones pr√°cticas\n",
    "- Toda la documentaci√≥n est√° lista para replicabilidad\n",
    "\n",
    "**¬°Todos los archivos est√°n guardados en tu Google Drive para acceso posterior!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
