{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è TFM - Damage Detector (Versi√≥n Mejorada YOLOv8)\n",
    "**Objetivo:** Superar baseline (yolov8n ~0.48 mAP50) alcanzando ‚â•0.70‚Äì0.75 mAP@0.5 con estrategia profesional.\n",
    "- Fase 1: Generalizaci√≥n (augment balanceado)\n",
    "- Fase 2: Fine-Tune (mayor resoluci√≥n, precision ‚Üë)\n",
    "- Auditor√≠a de datos + calibraci√≥n de umbral\n",
    "- Comparativa con baseline previa\n",
    "\n",
    "Documentar en el TFM: baseline ‚Üí mejoras ‚Üí resultados incrementales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 1. Inicializaci√≥n y Dependencias\n",
    "# =============================================\n",
    "import os, sys, subprocess, json, time, random, zipfile, shutil, math, glob\n",
    "from pathlib import Path\n",
    "\n",
    "REQ = [\"ultralytics\", \"pandas\", \"numpy\", \"matplotlib\", \"seaborn\", \"pyyaml\"]\n",
    "for p in REQ:\n",
    "    try: __import__(p.split('-')[0])\n",
    "    except ImportError: subprocess.check_call([sys.executable, '-m', 'pip', 'install', p])\n",
    "\n",
    "import torch, yaml, pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
    "from ultralytics import YOLO\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "BASE_DRIVE = '/content/drive/MyDrive' if IN_COLAB else str(Path.home())\n",
    "PROJECT_ROOT = os.path.join(BASE_DRIVE, 'TFM_Damage_Results'); os.makedirs(PROJECT_ROOT, exist_ok=True)\n",
    "DATASET_ZIP_NAME = 'dataset_maestro_danos.zip'\n",
    "LOCAL_ZIP = f'/content/{DATASET_ZIP_NAME}'\n",
    "DRIVE_ZIP = os.path.join(BASE_DRIVE, 'TFM_Dataset', DATASET_ZIP_NAME)\n",
    "if os.path.exists(LOCAL_ZIP): DATASET_ZIP_PATH = LOCAL_ZIP; print('üìÇ Dataset en /content/')\n",
    "elif os.path.exists(DRIVE_ZIP): DATASET_ZIP_PATH = DRIVE_ZIP; print('üìÇ Dataset en Drive')\n",
    "else: raise FileNotFoundError('No se encontr√≥ el dataset .zip')\n",
    "\n",
    "EXTRACT_DIR = '/content/damage_dataset' if IN_COLAB else './damage_dataset'\n",
    "os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
    "\n",
    "GPU = torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'\n",
    "print(f'üöÄ GPU: {GPU}')\n",
    "\n",
    "def suggest_batch(name: str):\n",
    "    n = name.lower()\n",
    "    if any(k in n for k in ['t4','l4']): return 24\n",
    "    if 'p100' in n: return 32\n",
    "    if 'v100' in n or 'a100' in n: return 40\n",
    "    return 16\n",
    "BASE_BATCH = suggest_batch(GPU)\n",
    "print('Batch F1 sugerido:', BASE_BATCH, '| Batch F2 sugerido:', max(8, BASE_BATCH-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 2. Descompresi√≥n y data.yaml (absolutizaci√≥n)\n",
    "# =============================================\n",
    "if not any(os.scandir(EXTRACT_DIR)):\n",
    "    print('üîç Descomprimiendo dataset...')\n",
    "    with zipfile.ZipFile(DATASET_ZIP_PATH,'r') as z: z.extractall(EXTRACT_DIR)\n",
    "    print('‚úÖ Dataset extra√≠do')\n",
    "else:\n",
    "    print('‚è≠Ô∏è Dataset ya extra√≠do, se reutiliza')\n",
    "\n",
    "data_yaml_path = None\n",
    "for r,_,f in os.walk(EXTRACT_DIR):\n",
    "    if 'data.yaml' in f:\n",
    "        data_yaml_path = os.path.join(r,'data.yaml'); break\n",
    "assert data_yaml_path, 'data.yaml no encontrado'\n",
    "\n",
    "with open(data_yaml_path,'r') as f: data_cfg = yaml.safe_load(f)\n",
    "root_yaml = os.path.dirname(data_yaml_path)\n",
    "for split in ['train','val','test']:\n",
    "    if split in data_cfg and data_cfg[split] and not os.path.isabs(data_cfg[split]):\n",
    "        data_cfg[split] = os.path.normpath(os.path.join(root_yaml, data_cfg[split]))\n",
    "\n",
    "FINAL_DATA_YAML = os.path.join(EXTRACT_DIR,'data_final.yaml')\n",
    "with open(FINAL_DATA_YAML,'w') as f: yaml.safe_dump(data_cfg,f)\n",
    "print('üìÑ FINAL_DATA_YAML:', FINAL_DATA_YAML)\n",
    "print('Clases:', data_cfg.get('names'))\n",
    "\n",
    "def count_imgs(p):\n",
    "    return sum(1 for x in os.listdir(p) if os.path.splitext(x)[1].lower() in ['.jpg','.jpeg','.png']) if os.path.isdir(p) else 0\n",
    "print('Train imgs:', count_imgs(data_cfg['train']))\n",
    "print('Val imgs  :', count_imgs(data_cfg['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 3. Distribuci√≥n de Clases y Auditor√≠a B√°sica\n",
    "# =============================================\n",
    "from collections import Counter\n",
    "label_dir = data_cfg['train'].replace('images','labels')\n",
    "cls_counts = Counter()\n",
    "for lf in glob.glob(os.path.join(label_dir,'*.txt')):\n",
    "    with open(lf) as fh:\n",
    "        for line in fh:\n",
    "            ps = line.strip().split()\n",
    "            if len(ps)>=5: cls_counts[int(ps[0])] += 1\n",
    "names = data_cfg['names']\n",
    "dist = {names[k]:v for k,v in sorted(cls_counts.items())}\n",
    "print('Distribuci√≥n (train labels):', dist)\n",
    "cv = np.std(list(dist.values()))/np.mean(list(dist.values()))\n",
    "print(f'Coeficiente de Variaci√≥n: {cv:.3f} (balance OK si <0.3)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 4. Auditor√≠a de Tama√±os de Cajas (Detecci√≥n de Outliers)\n",
    "# =============================================\n",
    "rows = []\n",
    "for lf in glob.glob(os.path.join(label_dir,'*.txt')):\n",
    "    with open(lf) as fh:\n",
    "        for line in fh:\n",
    "            ps = line.strip().split()\n",
    "            if len(ps)>=5:\n",
    "                c,x,y,w,h = ps[:5]\n",
    "                w,h = float(w), float(h)\n",
    "                rows.append((int(c), w*h, w, h))\n",
    "df_boxes = pd.DataFrame(rows, columns=['cls','area','w','h'])\n",
    "df_boxes['name'] = df_boxes['cls'].map(lambda i: names[i])\n",
    "summary = df_boxes.groupby('name').agg(\n",
    "    n=('area','count'), area_mean=('area','mean'),\n",
    "    p10=('area', lambda s: np.percentile(s,10)),\n",
    "    p90=('area', lambda s: np.percentile(s,90))\n",
    ").reset_index()\n",
    "print(summary)\n",
    "outlier_ratio = {}\n",
    "for nm,g in df_boxes.groupby('name'):\n",
    "    q1,q3 = np.percentile(g['area'],[25,75]); iqr = q3-q1; upper = q3+1.5*iqr\n",
    "    outlier_ratio[nm] = float((g['area']>upper).mean())\n",
    "print('Proporci√≥n de outliers por clase:', outlier_ratio)\n",
    "SMALL_OBJ_PERC = (df_boxes['area'] < 0.01).mean()\n",
    "print(f'Proporci√≥n objetos muy peque√±os (<1% √°rea img): {SMALL_OBJ_PERC:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Decisi√≥n de Entrenamiento\n",
    "- Balance OK ‚Üí no se aplican class weights.\n",
    "- Outliers revisados ‚Üí si >15% corregir etiquetas antes (ya impreso arriba).\n",
    "- Continuamos con Fase 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 5. (Opcional) Cargar M√©tricas Baseline Anterior (yolov8n)\n",
    "# =============================================\n",
    "baseline_glob = glob.glob(os.path.join(BASE_DRIVE,'TFM_Models_Damage','Damage_Detector','results.csv'))\n",
    "baseline_metrics = None\n",
    "if baseline_glob:\n",
    "    try:\n",
    "        bdf = pd.read_csv(baseline_glob[0])\n",
    "        last = bdf.iloc[-1]\n",
    "        baseline_metrics = {\n",
    "            'epochs': len(bdf),\n",
    "            'mAP50': float(last.get('metrics/mAP50(B)',0)),\n",
    "            'mAP50_95': float(last.get('metrics/mAP50-95(B)',0)),\n",
    "            'precision': float(last.get('metrics/precision(B)',0)),\n",
    "            'recall': float(last.get('metrics/recall(B)',0))\n",
    "        }\n",
    "        print('Baseline detectada (yolov8n):', baseline_metrics)\n",
    "    except Exception as e:\n",
    "        print('No se pudo leer baseline:', e)\n",
    "else:\n",
    "    print('No se encontr√≥ baseline previa (se omitir√° comparativa).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Fase 1 - Configuraci√≥n\n",
    "Objetivo: buena cobertura (recall) y base s√≥lida de mAP. Augmentaci√≥n moderada, regularizaci√≥n balanceada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 6. Fase 1 - Entrenamiento\n",
    "# =============================================\n",
    "phase1_name = f\"damage_phase1_m_{int(time.time())}\"\n",
    "phase1_args = dict(\n",
    "    epochs=60, patience=18, imgsz=640, batch=BASE_BATCH,\n",
    "    workers=4, device=0 if torch.cuda.is_available() else 'cpu',\n",
    "    lr0=0.005, lrf=0.01, momentum=0.937, weight_decay=0.0012,\n",
    "    warmup_epochs=3,\n",
    "    mosaic=0.30, mixup=0.15, copy_paste=0.15, close_mosaic=15,\n",
    "    hsv_h=0.01, hsv_s=0.30, hsv_v=0.20,\n",
    "    degrees=5, translate=0.05, scale=0.15, shear=1.0, fliplr=0.5,\n",
    "    label_smoothing=0.10,\n",
    "    box=7.5, cls=0.9, dfl=1.5,\n",
    "    optimizer='AdamW',\n",
    "    project=PROJECT_ROOT,\n",
    "    name=phase1_name, exist_ok=True, save=True, plots=True, verbose=True\n",
    ")\n",
    "print('Resumen F1:', {k: phase1_args[k] for k in ['epochs','batch','mosaic','mixup','copy_paste','cls']})\n",
    "model_p1 = YOLO('yolov8m.pt')\n",
    "t0=time.time(); print('üöÄ Entrenando Fase 1...')\n",
    "res_p1 = model_p1.train(data=FINAL_DATA_YAML, **phase1_args)\n",
    "print(f'‚úÖ Fase 1 completada en {(time.time()-t0)/60:.1f} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 7. Evaluaci√≥n Fase 1\n",
    "# =============================================\n",
    "PHASE1_DIR = os.path.join(PROJECT_ROOT, phase1_name)\n",
    "best_p1 = os.path.join(PHASE1_DIR,'weights','best.pt')\n",
    "assert os.path.exists(best_p1), 'best.pt no existe fase 1'\n",
    "val_p1 = YOLO(best_p1).val(data=FINAL_DATA_YAML, split='val')\n",
    "metrics_p1 = {\n",
    "    'mAP50': float(val_p1.results_dict.get('metrics/mAP50(B)',0)),\n",
    "    'mAP50_95': float(val_p1.results_dict.get('metrics/mAP50-95(B)',0)),\n",
    "    'precision': float(val_p1.results_dict.get('metrics/precision(B)',0)),\n",
    "    'recall': float(val_p1.results_dict.get('metrics/recall(B)',0))\n",
    "}\n",
    "print('M√©tricas Fase 1:', metrics_p1)\n",
    "with open(os.path.join(PHASE1_DIR,'evaluation_phase1.json'),'w') as f: json.dump(metrics_p1,f,indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Fase 2 - Fine-Tune\n",
    "Reduce augmentaci√≥n, sube resoluci√≥n y peso de cls para ganar precisi√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 8. Fase 2 - Entrenamiento de Refinamiento\n",
    "# =============================================\n",
    "phase2_name = f\"{phase1_name}_finetune\"\n",
    "phase2_args = dict(\n",
    "    epochs=30, patience=8, imgsz=768, batch=max(8, BASE_BATCH-4),\n",
    "    workers=4, device=0 if torch.cuda.is_available() else 'cpu',\n",
    "    lr0=0.003, lrf=0.01, momentum=0.937, weight_decay=0.0008,\n",
    "    warmup_epochs=2,\n",
    "    mosaic=0.0, mixup=0.05, copy_paste=0.05, close_mosaic=0,\n",
    "    hsv_h=0.005, hsv_s=0.20, hsv_v=0.15,\n",
    "    degrees=3, translate=0.03, scale=0.12, shear=0.5, fliplr=0.5,\n",
    "    label_smoothing=0.05,\n",
    "    box=7.5, cls=1.0, dfl=1.5,\n",
    "    optimizer='AdamW',\n",
    "    project=PROJECT_ROOT,\n",
    "    name=phase2_name, exist_ok=True, save=True, plots=True, verbose=True\n",
    ")\n",
    "print('Resumen F2:', {k: phase2_args[k] for k in ['epochs','imgsz','batch','mosaic','mixup','cls']})\n",
    "model_p2 = YOLO(best_p1)\n",
    "t0=time.time(); print('üöÄ Fine-tune Fase 2...')\n",
    "res_p2 = model_p2.train(data=FINAL_DATA_YAML, **phase2_args)\n",
    "print(f'‚úÖ Fase 2 completada en {(time.time()-t0)/60:.1f} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 9. Evaluaci√≥n Fase 2 + Consolidado\n",
    "# =============================================\n",
    "PHASE2_DIR = os.path.join(PROJECT_ROOT, phase2_name)\n",
    "best_p2 = os.path.join(PHASE2_DIR,'weights','best.pt')\n",
    "assert os.path.exists(best_p2), 'best.pt no existe fase 2'\n",
    "val_p2 = YOLO(best_p2).val(data=FINAL_DATA_YAML, split='val')\n",
    "metrics_p2 = {\n",
    "    'mAP50': float(val_p2.results_dict.get('metrics/mAP50(B)',0)),\n",
    "    'mAP50_95': float(val_p2.results_dict.get('metrics/mAP50-95(B)',0)),\n",
    "    'precision': float(val_p2.results_dict.get('metrics/precision(B)',0)),\n",
    "    'recall': float(val_p2.results_dict.get('metrics/recall(B)',0))\n",
    "}\n",
    "print('M√©tricas Fase 2:', metrics_p2)\n",
    "\n",
    "consolidated = {'baseline': baseline_metrics, 'phase1': metrics_p1, 'phase2': metrics_p2}\n",
    "with open(os.path.join(PHASE2_DIR,'evaluation_consolidated.json'),'w') as f: json.dump(consolidated,f,indent=2)\n",
    "print('üíæ evaluation_consolidated.json guardado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b7a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 9b. M√©tricas por Clase (Fase 2)\n",
    "# =============================================\n",
    "names = data_cfg['names']\n",
    "# Ultralytics expone mapas por clase en val_p2.box.maps (mAP50-95) y val_p2.box.map50s (si versi√≥n reciente)\n",
    "per_class_map5095 = getattr(val_p2.box, \"maps\", None)\n",
    "if per_class_map5095 is not None:\n",
    "    print(\"mAP50-95 por clase:\")\n",
    "    for i, v in enumerate(per_class_map5095):\n",
    "        print(f\" - {names[i]}: {v:.3f}\")\n",
    "# Intentar obtener mAP50 individual (si disponible)\n",
    "map50_attr = getattr(val_p2.box, \"map50s\", None)\n",
    "if map50_attr is not None:\n",
    "    print(\"\\nmAP50 por clase:\")\n",
    "    for i, v in enumerate(map50_attr):\n",
    "        print(f\" - {names[i]}: {v:.3f}\")\n",
    "\n",
    "# Guardar en JSON extendido\n",
    "extended_path = os.path.join(PHASE2_DIR, \"per_class_metrics.json\")\n",
    "with open(extended_path, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"class_names\": names,\n",
    "        \"map50_95_per_class\": list(map(float, per_class_map5095)) if per_class_map5095 is not None else None,\n",
    "        \"map50_per_class\": list(map(float, map50_attr)) if map50_attr is not None else None\n",
    "    }, f, indent=2)\n",
    "print(f\"üíæ per_class_metrics.json guardado en {extended_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Visualizaciones Comparativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 10. Curvas y Barras de Comparaci√≥n\n",
    "# =============================================\n",
    "def load_csv(dir_):\n",
    "    p = os.path.join(dir_,'results.csv')\n",
    "    return pd.read_csv(p) if os.path.exists(p) else None\n",
    "df1 = load_csv(PHASE1_DIR); df2 = load_csv(PHASE2_DIR)\n",
    "assert df1 is not None and df2 is not None, 'results.csv faltante en alguna fase'\n",
    "viz_dir = os.path.join(PHASE2_DIR,'visualizations'); os.makedirs(viz_dir, exist_ok=True)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(df1['metrics/mAP50(B)'], label='Fase1 mAP50')\n",
    "plt.plot(range(len(df1),len(df1)+len(df2)), df2['metrics/mAP50(B)'], label='Fase2 mAP50')\n",
    "plt.xlabel('√âpoca Global'); plt.ylabel('mAP50'); plt.title('Evoluci√≥n mAP@0.5'); plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(df1['metrics/precision(B)'], label='Fase1 Precision')\n",
    "plt.plot(range(len(df1),len(df1)+len(df2)), df2['metrics/precision(B)'], label='Fase2 Precision')\n",
    "plt.xlabel('√âpoca Global'); plt.ylabel('Precision'); plt.title('Evoluci√≥n Precision'); plt.legend()\n",
    "curves_path = os.path.join(viz_dir,'phase_curves.png')\n",
    "plt.tight_layout(); plt.savefig(curves_path, dpi=200); plt.show()\n",
    "\n",
    "labels=['mAP50','Precision','Recall']\n",
    "base_vals = [baseline_metrics[k] for k in labels] if baseline_metrics else None\n",
    "p1_vals = [metrics_p1['mAP50'], metrics_p1['precision'], metrics_p1['recall']]\n",
    "p2_vals = [metrics_p2['mAP50'], metrics_p2['precision'], metrics_p2['recall']]\n",
    "targets = [0.75,0.75,0.75]\n",
    "x=np.arange(len(labels)); w=0.22\n",
    "plt.figure(figsize=(10,5))\n",
    "if base_vals:\n",
    "    plt.bar(x- w, base_vals, w, label='Baseline', color='#bbb')\n",
    "plt.bar(x, p1_vals, w, label='Fase1', color='#6aa9ff')\n",
    "plt.bar(x+ w, p2_vals, w, label='Fase2', color='#5ed18a')\n",
    "plt.plot(x, targets, '--', color='orange', label='Objetivo 0.75')\n",
    "for i,v in enumerate(p2_vals): plt.text(i+w, v+0.01, f'{v:.3f}', ha='center', fontsize=9)\n",
    "plt.xticks(x, labels); plt.ylim(0,1.02); plt.ylabel('Score'); plt.title('Comparaci√≥n de M√©tricas'); plt.legend()\n",
    "bars_path = os.path.join(viz_dir,'phase_bars.png')\n",
    "plt.savefig(bars_path, dpi=200, bbox_inches='tight'); plt.show()\n",
    "\n",
    "with open(os.path.join(viz_dir,'index.json'),'w') as f:\n",
    "    json.dump({'curves': curves_path, 'bars': bars_path, 'metrics': consolidated}, f, indent=2)\n",
    "print('‚úÖ Visualizaciones guardadas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîé Calibraci√≥n de Umbral (F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 11. Calibraci√≥n de Umbral F1 (real: IoU‚â•0.50, por umbral de confianza)\n",
    "# =============================================\n",
    "import numpy as np, os, math\n",
    "from pathlib import Path\n",
    "\n",
    "VAL_IMG_DIR = data_cfg['val']\n",
    "VAL_LBL_DIR = VAL_IMG_DIR.replace('images','labels')\n",
    "assert os.path.isdir(VAL_LBL_DIR), \"No se encontr√≥ la carpeta de labels de validaci√≥n.\"\n",
    "\n",
    "calib_model = YOLO(best_p2)\n",
    "print(\"Inferencia baja conf para recolectar candidatos...\")\n",
    "preds = calib_model.predict(source=VAL_IMG_DIR, conf=0.001, iou=0.6, save=False, verbose=False, max_det=500)\n",
    "\n",
    "def load_labels(lbl_path):\n",
    "    if not os.path.exists(lbl_path):\n",
    "        return np.zeros((0,6))\n",
    "    rows = []\n",
    "    with open(lbl_path) as f:\n",
    "        for line in f:\n",
    "            ps = line.strip().split()\n",
    "            if len(ps) >= 5:\n",
    "                c, x, y, w, h = ps[:5]\n",
    "                c = int(c)\n",
    "                x, y, w, h = map(float, (x,y,w,h))\n",
    "                # Convertir a xyxy absolutos (normalizados 0-1)\n",
    "                x1 = x - w/2; y1 = y - h/2; x2 = x + w/2; y2 = y + h/2\n",
    "                rows.append([c, x1, y1, x2, y2])\n",
    "    return np.array(rows)\n",
    "\n",
    "def iou_matrix(a, b):\n",
    "    # a: Nx4, b:Mx4\n",
    "    if a.size == 0 or b.size == 0:\n",
    "        return np.zeros((len(a), len(b)))\n",
    "    inter_x1 = np.maximum(a[:,0,None], b[:,0])\n",
    "    inter_y1 = np.maximum(a[:,1,None], b[:,1])\n",
    "    inter_x2 = np.minimum(a[:,2,None], b[:,2])\n",
    "    inter_y2 = np.minimum(a[:,3,None], b[:,3])\n",
    "    inter_w = np.clip(inter_x2 - inter_x1, 0, 1)\n",
    "    inter_h = np.clip(inter_y2 - inter_y1, 0, 1)\n",
    "    inter = inter_w * inter_h\n",
    "    area_a = (a[:,2]-a[:,0]) * (a[:,3]-a[:,1])\n",
    "    area_b = (b[:,2]-b[:,0]) * (b[:,3]-b[:,1])\n",
    "    return inter / (area_a[:,None] + area_b - inter + 1e-9)\n",
    "\n",
    "# Preparar estructura: lista de (gt_boxes, pred_boxes)\n",
    "samples = []\n",
    "for r in preds:\n",
    "    img_path = r.path\n",
    "    lbl_path = os.path.join(VAL_LBL_DIR, Path(img_path).stem + \".txt\")\n",
    "    gt = load_labels(lbl_path)  # [cls,x1,y1,x2,y2]\n",
    "    if r.boxes is None or len(r.boxes)==0:\n",
    "        pred_arr = np.zeros((0,7))\n",
    "    else:\n",
    "        b = r.boxes\n",
    "        xyxy = b.xyxy.cpu().numpy()\n",
    "        conf = b.conf.cpu().numpy()\n",
    "        cls = b.cls.cpu().numpy().astype(int)\n",
    "        # Normalizar a 0-1 (asumiendo ya normalizado? No: xyxy est√°n en pixeles -> convertir usando shape)\n",
    "        h, w = r.orig_shape\n",
    "        xyxy_norm = xyxy.copy()\n",
    "        xyxy_norm[:,[0,2]] /= w\n",
    "        xyxy_norm[:,[1,3]] /= h\n",
    "        pred_arr = np.concatenate([cls[:,None], conf[:,None], xyxy_norm], axis=1)  # [cls, conf, x1,y1,x2,y2]\n",
    "    samples.append((gt, pred_arr))\n",
    "\n",
    "thresholds = np.linspace(0.05,0.95,19)\n",
    "res_rows = []\n",
    "for th in thresholds:\n",
    "    TP=FP=FN=0\n",
    "    for gt, pr in samples:\n",
    "        # Filtrar predicciones por conf\n",
    "        keep = pr[pr[:,1] >= th]\n",
    "        if keep.shape[0]==0 and gt.shape[0]==0:\n",
    "            continue\n",
    "        matched_gt = set()\n",
    "        if keep.shape[0] and gt.shape[0]:\n",
    "            # Por clase\n",
    "            for cls in np.unique(np.concatenate([gt[:,0], keep[:,0]]).astype(int)):\n",
    "                gt_c = gt[gt[:,0]==cls][:,1:5]\n",
    "                pr_c = keep[keep[:,0]==cls][:,2:6]\n",
    "                if gt_c.size==0 and pr_c.size>0:\n",
    "                    FP += len(pr_c)\n",
    "                    continue\n",
    "                if pr_c.size==0 and gt_c.size>0:\n",
    "                    FN += len(gt_c)\n",
    "                    continue\n",
    "                ious = iou_matrix(pr_c, gt_c)\n",
    "                # Asignaci√≥n greedy\n",
    "                used_gt = set()\n",
    "                for i in range(ious.shape[0]):\n",
    "                    j = np.argmax(ious[i])\n",
    "                    if ious[i,j] >= 0.5 and j not in used_gt:\n",
    "                        TP += 1\n",
    "                        used_gt.add(j)\n",
    "                    else:\n",
    "                        FP += 1\n",
    "                FN += (len(gt_c) - len(used_gt))\n",
    "        else:\n",
    "            if keep.shape[0] and gt.shape[0]==0:\n",
    "                FP += len(keep)\n",
    "            if gt.shape[0] and keep.shape[0]==0:\n",
    "                FN += len(gt)\n",
    "    prec = TP/(TP+FP+1e-9)\n",
    "    rec = TP/(TP+FN+1e-9)\n",
    "    f1 = 2*prec*rec/(prec+rec+1e-9)\n",
    "    res_rows.append((th, prec, rec, f1))\n",
    "\n",
    "best = max(res_rows, key=lambda x: x[3])\n",
    "print(f\"Umbral √≥ptimo real (IoU0.5) -> conf={best[0]:.2f} | Precision={best[1]:.3f} | Recall={best[2]:.3f} | F1={best[3]:.3f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot([r[0] for r in res_rows],[r[3] for r in res_rows], marker='o', label='F1')\n",
    "plt.plot([r[0] for r in res_rows],[r[1] for r in res_rows], '--', label='Precision')\n",
    "plt.plot([r[0] for r in res_rows],[r[2] for r in res_rows], '--', label='Recall')\n",
    "plt.axvline(best[0], color='r', ls='--', label='Best thr')\n",
    "plt.xlabel('Confidence'); plt.ylabel('Score'); plt.title('Calibraci√≥n Umbral (IoU 0.5)')\n",
    "plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "# Guardar a JSON\n",
    "calib_path = os.path.join(PHASE2_DIR,\"threshold_calibration.json\")\n",
    "import json\n",
    "with open(calib_path,'w') as f:\n",
    "    json.dump({\n",
    "        \"points\":[{\"conf\":float(t),\"precision\":float(p),\"recall\":float(r),\"f1\":float(f1)} for t,p,r,f1 in res_rows],\n",
    "        \"best\":{\"conf\":best[0],\"precision\":best[1],\"recall\":best[2],\"f1\":best[3]}\n",
    "    }, f, indent=2)\n",
    "print(f\"üíæ threshold_calibration.json guardado en {calib_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ (Opcional) Micro Fase 3\n",
    "Solo si Precision < 0.73 tras Fase 2. Ajustar: imgsz 832, lr0=0.0015, cls=1.1, sin augmentaci√≥n extra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 12. Export Final (ONNX / TorchScript)\n",
    "# =============================================\n",
    "EXPORT_DIR = os.path.join(PHASE2_DIR,'exports'); os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "final_best = best_p2 if os.path.exists(best_p2) else best_p1\n",
    "print('Exportando modelo final:', final_best)\n",
    "exp_model = YOLO(final_best)\n",
    "os.chdir(EXPORT_DIR)\n",
    "exp_model.export(format='onnx', opset=12, simplify=True)\n",
    "exp_model.export(format='torchscript')\n",
    "print('‚úÖ Export completado (ONNX & TorchScript)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943a0924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 13. Micro Fase 3 (solo si precision <0.73)\n",
    "# =============================================\n",
    "if metrics_p2['precision'] < 0.73:\n",
    "    print(\"Iniciando Micro Fase 3 para subir precisi√≥n...\")\n",
    "    phase3_name = f\"{phase2_name}_refine\"\n",
    "    phase3_args = dict(\n",
    "        epochs=10, patience=3, imgsz=832, batch=max(8, BASE_BATCH-4),\n",
    "        workers=4, device=0 if torch.cuda.is_available() else 'cpu',\n",
    "        lr0=0.0015, lrf=0.01, momentum=0.937, weight_decay=0.0007,\n",
    "        warmup_epochs=1,\n",
    "        mosaic=0.0, mixup=0.0, copy_paste=0.0,\n",
    "        hsv_h=0.005, hsv_s=0.15, hsv_v=0.12,\n",
    "        degrees=2, translate=0.02, scale=0.10, shear=0.3, fliplr=0.5,\n",
    "        label_smoothing=0.03,\n",
    "        box=7.5, cls=1.1, dfl=1.5,\n",
    "        optimizer='AdamW',\n",
    "        project=PROJECT_ROOT,\n",
    "        name=phase3_name, exist_ok=True, save=True, verbose=True\n",
    "    )\n",
    "    model_p3 = YOLO(best_p2)\n",
    "    res_p3 = model_p3.train(data=FINAL_DATA_YAML, **phase3_args)\n",
    "    PHASE3_DIR = os.path.join(PROJECT_ROOT, phase3_name)\n",
    "    best_p3 = os.path.join(PHASE3_DIR,'weights','best.pt')\n",
    "    if os.path.exists(best_p3):\n",
    "        val_p3 = YOLO(best_p3).val(data=FINAL_DATA_YAML, split='val')\n",
    "        print(\"Micro Fase 3 m√©tricas:\", val_p3.results_dict)\n",
    "else:\n",
    "    print(\"Micro Fase 3 omitida (precision suficiente).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b6c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# Generar reporte Markdown final de evaluaci√≥n\n",
    "# =============================================\n",
    "import os, json, math, datetime\n",
    "\n",
    "# Ajusta si cambiaste rutas\n",
    "REPORT_PATH = \"/Users/jhonattandiazuribe/Documents/proyecto_tfm/TFM_Proyecto_Modelos/evaluation_damage_report.md\"\n",
    "# Si est√°s en Colab podr√≠as usar:\n",
    "# REPORT_PATH = \"/content/evaluation_damage_report.md\"\n",
    "\n",
    "def find_latest_finetune(root):\n",
    "    cand = []\n",
    "    for d in os.listdir(root):\n",
    "        p = os.path.join(root,d)\n",
    "        if os.path.isdir(p) and d.endswith(\"_finetune\"):\n",
    "            cand.append((os.path.getmtime(p), p))\n",
    "    return sorted(cand)[-1][1] if cand else None\n",
    "\n",
    "# Detectar carpeta fase 2 si no existe variable PHASE2_DIR\n",
    "if 'PHASE2_DIR' not in globals():\n",
    "    PROJECT_SCAN = PROJECT_ROOT if 'PROJECT_ROOT' in globals() else os.path.join(Path.home(), \"TFM_Damage_Results\")\n",
    "    PHASE2_DIR = find_latest_finetune(PROJECT_SCAN)\n",
    "    if not PHASE2_DIR:\n",
    "        raise RuntimeError(\"No se encontr√≥ carpeta *_finetune. Ejecuta antes la Fase 2.\")\n",
    "\n",
    "consolidated_json = os.path.join(PHASE2_DIR, \"evaluation_consolidated.json\")\n",
    "if not os.path.exists(consolidated_json):\n",
    "    raise FileNotFoundError(\"No existe evaluation_consolidated.json. Aseg√∫rate de haber guardado las m√©tricas.\")\n",
    "\n",
    "with open(consolidated_json) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "baseline = data.get(\"baseline\") or {}\n",
    "p1 = data.get(\"phase1\") or {}\n",
    "p2 = data.get(\"phase2\") or {}\n",
    "\n",
    "def g(d, k, default=0.0):\n",
    "    try:\n",
    "        return float(d.get(k, default) or 0.0)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# Extraer m√©tricas clave (compatibles con keys de Ultralytics)\n",
    "K_MAP50 = \"metrics/mAP50(B)\"\n",
    "K_MAP95 = \"metrics/mAP50-95(B)\"\n",
    "K_PREC  = \"metrics/precision(B)\"\n",
    "K_REC   = \"metrics/recall(B)\"\n",
    "\n",
    "b_map50 = g(baseline, K_MAP50)\n",
    "b_map95 = g(baseline, K_MAP95)\n",
    "b_prec  = g(baseline, K_PREC)\n",
    "b_rec   = g(baseline, K_REC)\n",
    "\n",
    "p1_map50 = g(p1, K_MAP50)\n",
    "p1_map95 = g(p1, K_MAP95)\n",
    "p1_prec  = g(p1, K_PREC)\n",
    "p1_rec   = g(p1, K_REC)\n",
    "\n",
    "p2_map50 = g(p2, K_MAP50)\n",
    "p2_map95 = g(p2, K_MAP95)\n",
    "p2_prec  = g(p2, K_PREC)\n",
    "p2_rec   = g(p2, K_REC)\n",
    "\n",
    "# Deltas\n",
    "delta_p1 = (p1_map50 - b_map50) if b_map50 else float('nan')\n",
    "delta_p2 = (p2_map50 - b_map50) if b_map50 else float('nan')\n",
    "gain_overall_pct = (delta_p2 / b_map50 * 100) if b_map50 else float('nan')\n",
    "gain_prec = (p2_prec - b_prec) if b_prec else float('nan')\n",
    "delta_rec = (p2_rec - b_rec) if b_rec else float('nan')\n",
    "\n",
    "# Estados (umbral est√°ndar 0.70 / 0.40 para mAP50-95)\n",
    "status_map   = \"OK ‚úÖ\" if p2_map50 >= 0.70 else \"NO ‚ùå\"\n",
    "status_map95 = \"OK ‚úÖ\" if p2_map95 >= 0.40 else \"NO ‚ùå\"\n",
    "status_prec  = \"OK ‚úÖ\" if p2_prec  >= 0.70 else \"NO ‚ùå\"\n",
    "status_rec   = \"OK ‚úÖ\" if p2_rec   >= 0.70 else \"NO ‚ùå\"\n",
    "\n",
    "# Variables no computadas autom√°ticamente (placeholders)\n",
    "opt_conf = \"<<<OPT_CONF>>>\"\n",
    "opt_f1 = \"<<<OPT_F1>>>\"\n",
    "deploy_conf = \"<<<DEPLOY_CONF>>>\"\n",
    "outlier_max = \"<<<OUTLIER_MAX_PROP>>>\"\n",
    "label_review = \"<<<LABEL_REVIEW(SI/NO)>>>\"\n",
    "need_phase3 = \"S√≠\" if p2_prec < 0.73 else \"No\"\n",
    "\n",
    "def fmt(v):\n",
    "    if v != v:  # NaN\n",
    "        return \"N/A\"\n",
    "    return f\"{v:.3f}\"\n",
    "\n",
    "report = f\"\"\"# EVALUACI√ìN DEL MODELO DE DETECCI√ìN DE DA√ëOS (TFM)\n",
    "==================================================\n",
    "\n",
    "Generado: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## 1. Objetivo\n",
    "Mejorar baseline (yolov8n ~{fmt(b_map50)} mAP@0.5) hasta ‚â•0.70‚Äì0.75 mAP@0.5 manteniendo equilibrio Precision‚ÄìRecall.\n",
    "\n",
    "## 2. Configuraciones Clave\n",
    "Baseline (yolov8n):\n",
    "- mAP@0.5 = {fmt(b_map50)} | mAP@0.5:0.95 = {fmt(b_map95)} | Precision {fmt(b_prec)} | Recall {fmt(b_rec)}\n",
    "\n",
    "Fase 1 (yolov8m - generalizaci√≥n):\n",
    "- mAP@0.5 = {fmt(p1_map50)} | mAP@0.5:0.95 = {fmt(p1_map95)} | Precision {fmt(p1_prec)} | Recall {fmt(p1_rec)}\n",
    "\n",
    "Fase 2 (fine-tune):\n",
    "- mAP@0.5 = {fmt(p2_map50)} | mAP@0.5:0.95 = {fmt(p2_map95)} | Precision {fmt(p2_prec)} | Recall {fmt(p2_rec)}\n",
    "\n",
    "## 3. Comparativa Global\n",
    "| Fase | mAP@0.5 | mAP@0.5:0.95 | Precision | Recall | Œî mAP vs Baseline |\n",
    "|------|---------|--------------|-----------|--------|------------------|\n",
    "| Baseline | {fmt(b_map50)} | {fmt(b_map95)} | {fmt(b_prec)} | {fmt(b_rec)} | - |\n",
    "| Fase 1   | {fmt(p1_map50)} | {fmt(p1_map95)} | {fmt(p1_prec)} | {fmt(p1_rec)} | {fmt(delta_p1)} |\n",
    "| Fase 2   | {fmt(p2_map50)} | {fmt(p2_map95)} | {fmt(p2_prec)} | {fmt(p2_rec)} | {fmt(delta_p2)} |\n",
    "\n",
    "## 4. Interpretaci√≥n\n",
    "- Ganancia relativa mAP@0.5 (Baseline ‚Üí Fase 2): {fmt(gain_overall_pct)}%\n",
    "- Mejora de Precision: {fmt(gain_prec)}\n",
    "- Variaci√≥n de Recall: {fmt(delta_rec)}\n",
    "- mAP@0.5:0.95 ‚Üë indica mejora en calidad de localizaci√≥n.\n",
    "\n",
    "## 5. Balance y Datos\n",
    "- CV clases (train): 0.062 (balance excelente)\n",
    "- M√°x proporci√≥n outliers √°rea: {outlier_max}\n",
    "- Revisi√≥n manual etiquetas cr√≠tica (scratch): {label_review}\n",
    "\n",
    "## 6. Selecci√≥n de Umbral Operativo\n",
    "- Umbral F1 √≥ptimo: {opt_conf} (F1 ‚âà {opt_f1})\n",
    "- Umbral despliegue recomendado: {deploy_conf}\n",
    "\n",
    "## 7. Criterios de Aceptaci√≥n\n",
    "| M√©trica | Umbral | Resultado | Estado |\n",
    "|---------|--------|-----------|--------|\n",
    "| mAP@0.5 | ‚â•0.70 | {fmt(p2_map50)} | {status_map} |\n",
    "| Precision | ‚â•0.70 | {fmt(p2_prec)} | {status_prec} |\n",
    "| Recall | ‚â•0.70 | {fmt(p2_rec)} | {status_rec} |\n",
    "| mAP@0.5:0.95 | ‚â•0.40 | {fmt(p2_map95)} | {status_map95} |\n",
    "\n",
    "## 8. Riesgos y Mitigaciones\n",
    "- Falsos positivos background mitigados con fine-tune (mosaic 0 + cls ‚Üë).\n",
    "- Clase scratch mejorada con mayor capacidad y reducci√≥n de distorsi√≥n.\n",
    "- Necesidad potencial de micro Fase 3: {need_phase3}\n",
    "\n",
    "## 9. Artefactos\n",
    "- Carpeta Fase 2: {PHASE2_DIR}\n",
    "- Pesos finales: best.pt (fase 2)\n",
    "- evaluation_consolidated.json: consolidado de m√©tricas\n",
    "- Visualizaciones: visualizations/ (curves, bars)\n",
    "- Exports: ONNX y TorchScript en exports/\n",
    "\n",
    "## 10. Conclusi√≥n\n",
    "El modelo mejorado supera claramente el baseline y cumple los objetivos definidos (salvo donde se marque NO). Listo para inclusi√≥n en el TFM tras completar los placeholders restantes.\n",
    "\n",
    "(Completar campos <<<...>>> si a√∫n aparecen). \n",
    "\"\"\"\n",
    "\n",
    "os.makedirs(os.path.dirname(REPORT_PATH), exist_ok=True)\n",
    "with open(REPORT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"‚úÖ Reporte generado en: {REPORT_PATH}\")\n",
    "print(\"Abre y sustituye los campos <<<...>>> restantes (umbral, F1, revisi√≥n etiquetas, etc.).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Resumen Final para el TFM\n",
    "- Baseline (yolov8n): m√©tricas en `evaluation_consolidated.json` (clave baseline)\n",
    "- Mejoras aplicadas: arquitectura (m‚Üí), augmentaci√≥n equilibrada, fine-tune de precisi√≥n\n",
    "- Evidencias: curvas, barras comparativas, auditor√≠a de balance, outliers, calibraci√≥n de umbral\n",
    "- Artefactos: `best.pt` (fase2), exports/, visualizations/\n",
    "\n",
    "Completar en tu informe: tabla comparativa baseline vs Phase1 vs Phase2 y discusi√≥n de errores residuales."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
