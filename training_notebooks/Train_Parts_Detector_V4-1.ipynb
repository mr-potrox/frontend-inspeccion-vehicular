{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": { "name": "python3", "display_name": "Python 3" },
  "language_info": { "name": "python", "version": "3.x" }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöó TFM - Entrenamiento YOLOv8 (Detecci√≥n Partes Vehiculares)\n",
    "**Estrategia:** Entrenamiento en 2 fases para subir mAP@0.5 ‚â• 0.75 y Precision ‚â• 0.75 manteniendo Recall ‚â• 0.75.\n",
    "Fase 1: Regularizaci√≥n fuerte (generalizaci√≥n).  \n",
    "Fase 2: Fine-tune de precisi√≥n (resoluci√≥n ‚Üë, augment ‚Üì)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================\n",
    "# 1. Inicializaci√≥n e Instalaci√≥n\n",
    "# =============================================\n",
    "import os, sys, subprocess, json, time, zipfile, math, random, gc\n",
    "from pathlib import Path\n",
    "\n",
    "REQ = [\"ultralytics\", \"pandas\", \"numpy\", \"matplotlib\", \"seaborn\", \"pyyaml\"]\n",
    "for p in REQ:\n",
    "    try: __import__(p.split('-')[0])\n",
    "    except ImportError: subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", p])\n",
    "\n",
    "import torch, yaml, pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
    "from ultralytics import YOLO\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "# ------------ PAR√ÅMETROS PRINCIPALES (EDITABLES) ------------\n",
    "MODEL_SIZE = 'm'              # n / s / m / l / x\n",
    "PHASE1_EPOCHS = 60\n",
    "PHASE1_PATIENCE = 20\n",
    "PHASE2_EPOCHS = 35            # se frenar√° antes con patience\n",
    "PHASE2_PATIENCE = 8\n",
    "IMG_SIZE_PHASE1 = 640\n",
    "IMG_SIZE_PHASE2 = 768         # subir para refinar boxes\n",
    "PROJECT_NAME = 'TFM_Resultados'\n",
    "DATASET_ZIP_NAME = 'dataset_vehicular.zip'\n",
    "BASE_DRIVE = '/content/drive/MyDrive' if IN_COLAB else str(Path.home())\n",
    "SEED = 42\n",
    "\n",
    "# ------------ RUTAS GENERALES ------------\n",
    "DATASET_ZIP_PATH = os.path.join(BASE_DRIVE, 'TFM_Dataset', DATASET_ZIP_NAME)\n",
    "DATA_EXTRACT_DIR = '/content/dataset_extracted' if IN_COLAB else './dataset_extracted'\n",
    "RESULTS_ROOT = os.path.join(BASE_DRIVE, PROJECT_NAME)\n",
    "os.makedirs(DATA_EXTRACT_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_ROOT, exist_ok=True)\n",
    "\n",
    "def set_seed(s=SEED):\n",
    "    random.seed(s); np.random.seed(s); torch.manual_seed(s)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(s)\n",
    "set_seed()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    GPU_NAME = torch.cuda.get_device_name(0)\n",
    "    VRAM_GB = torch.cuda.get_device_properties(0).total_memory/1024**3\n",
    "else:\n",
    "    GPU_NAME, VRAM_GB = 'CPU', 0\n",
    "print(f\"üöÄ GPU: {GPU_NAME} | VRAM: {VRAM_GB:.1f} GB\")\n",
    "\n",
    "def suggest_batch(name:str):\n",
    "    n = name.lower()\n",
    "    if 't4' in n: return 24\n",
    "    if 'p100' in n: return 32\n",
    "    if 'v100' in n or 'a100' in n: return 40\n",
    "    return 16\n",
    "BASE_BATCH = suggest_batch(GPU_NAME)\n",
    "print(f\"üì¶ Batch sugerido Fase1: {BASE_BATCH}\")\n",
    "print(f\"üì¶ Batch sugerido Fase2: {max(8, BASE_BATCH-4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================\n",
    "# 2. Dataset: Descompresi√≥n + Validaci√≥n\n",
    "# =============================================\n",
    "assert os.path.exists(DATASET_ZIP_PATH), f\"ZIP no encontrado: {DATASET_ZIP_PATH}\"\n",
    "print(f\"üì¶ Descomprimiendo dataset: {DATASET_ZIP_PATH}\")\n",
    "with zipfile.ZipFile(DATASET_ZIP_PATH,'r') as z: z.extractall(DATA_EXTRACT_DIR)\n",
    "print(f\"‚úÖ Extra√≠do en: {DATA_EXTRACT_DIR}\")\n",
    "\n",
    "# Localizar data.yaml\n",
    "data_yaml = None\n",
    "for r,_,files in os.walk(DATA_EXTRACT_DIR):\n",
    "    if 'data.yaml' in files:\n",
    "        data_yaml = os.path.join(r,'data.yaml'); break\n",
    "assert data_yaml, 'data.yaml no encontrado'\n",
    "print(f\"üìÑ data.yaml: {data_yaml}\")\n",
    "\n",
    "with open(data_yaml,'r') as f: data_cfg = yaml.safe_load(f)\n",
    "root_yaml = os.path.dirname(data_yaml)\n",
    "for k in ['train','val','test']:\n",
    "    if k in data_cfg and data_cfg[k] and not os.path.isabs(data_cfg[k]):\n",
    "        data_cfg[k] = os.path.normpath(os.path.join(root_yaml, data_cfg[k]))\n",
    "\n",
    "FINAL_DATA_YAML = os.path.join(DATA_EXTRACT_DIR,'data_final.yaml')\n",
    "with open(FINAL_DATA_YAML,'w') as f: yaml.safe_dump(data_cfg,f)\n",
    "print(\"üîç Dataset Config:\")\n",
    "print(' nc:', data_cfg.get('nc'))\n",
    "print(' names:', data_cfg.get('names'))\n",
    "\n",
    "def count_images(p):\n",
    "    if not p or not os.path.exists(p): return 0\n",
    "    exts = {'.jpg','.jpeg','.png'}\n",
    "    return sum(1 for f in os.listdir(p) if os.path.splitext(f)[1].lower() in exts)\n",
    "print(' üñº Train images:', count_images(data_cfg.get('train','')))\n",
    "print(' üñº Val images  :', count_images(data_cfg.get('val','')))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================\n",
    "# 3. Fase 1 - Configuraci√≥n y Entrenamiento\n",
    "# =============================================\n",
    "phase1_name = f\"veh_parts_phase1_{MODEL_SIZE}_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "PHASE1_DIR = os.path.join(RESULTS_ROOT, phase1_name)\n",
    "os.makedirs(PHASE1_DIR, exist_ok=True)\n",
    "\n",
    "phase1_args = dict(\n",
    "    epochs=PHASE1_EPOCHS,\n",
    "    patience=PHASE1_PATIENCE,\n",
    "    imgsz=IMG_SIZE_PHASE1,\n",
    "    batch=BASE_BATCH,\n",
    "    workers=4,\n",
    "    device=0 if torch.cuda.is_available() else 'cpu',\n",
    "    amp=True,\n",
    "    cache=True,\n",
    "    lr0=0.005,\n",
    "    lrf=0.01,\n",
    "    momentum=0.937,\n",
    "    weight_decay=0.0015,\n",
    "    warmup_epochs=3,\n",
    "    label_smoothing=0.10,\n",
    "    hsv_h=0.01, hsv_s=0.30, hsv_v=0.20,\n",
    "    degrees=5, translate=0.05, scale=0.15, shear=1.0,\n",
    "    perspective=0.0, flipud=0.0, fliplr=0.5,\n",
    "    mosaic=0.30, mixup=0.20, copy_paste=0.25, close_mosaic=20,\n",
    "    box=7.5, cls=0.9, dfl=1.5,\n",
    "    optimizer='AdamW',\n",
    "    project=RESULTS_ROOT,\n",
    "    name=phase1_name,\n",
    "    save=True, save_period=10, plots=True, exist_ok=True, verbose=True\n",
    ")\n",
    "print(\"‚öôÔ∏è Config Fase 1 (resumen):\")\n",
    "print(json.dumps({k: phase1_args[k] for k in ['epochs','batch','patience','mosaic','mixup','copy_paste','cls']}, indent=2))\n",
    "\n",
    "base_model = f\"yolov8{MODEL_SIZE}.pt\"\n",
    "print(f\"üì• Cargando modelo base: {base_model}\")\n",
    "model_phase1 = YOLO(base_model)\n",
    "t0 = time.time()\n",
    "print(\"üöÄ Entrenando Fase 1...\")\n",
    "res_phase1 = model_phase1.train(data=FINAL_DATA_YAML, **phase1_args)\n",
    "t1 = (time.time()-t0)/60\n",
    "print(f\"‚úÖ Fase 1 completada en {t1:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================\n",
    "# 4. Monitoreo Opcional (ejecutar aparte si se desea)\n",
    "# =============================================\n",
    "import pandas as pd, time\n",
    "def monitor(experiment_dir, interval=25):\n",
    "    csv_path = os.path.join(experiment_dir,'results.csv')\n",
    "    last = -1\n",
    "    print(f\"üëÅÔ∏è Monitoreando: {experiment_dir}\")\n",
    "    while True:\n",
    "        if os.path.exists(csv_path):\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path)\n",
    "                if len(df)>0 and len(df)!=last:\n",
    "                    last=len(df)\n",
    "                    print(f\"√âpoca {last} | mAP50={df['metrics/mAP50(B)'].iloc[-1]:.3f} | P={df['metrics/precision(B)'].iloc[-1]:.3f} | R={df['metrics/recall(B)'].iloc[-1]:.3f}\")\n",
    "            except Exception as e:\n",
    "                print('Lectura error:', e)\n",
    "        else:\n",
    "            print('Esperando results.csv...')\n",
    "        time.sleep(interval)\n",
    "\n",
    "# monitor(PHASE1_DIR)  # Descomentar si se quiere usar mientras entrena"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================\n",
    "# 5. Evaluaci√≥n Fase 1\n",
    "# =============================================\n",
    "best_phase1 = os.path.join(PHASE1_DIR,'weights','best.pt')\n",
    "assert os.path.exists(best_phase1), 'best.pt Fase1 no encontrado'\n",
    "model_eval1 = YOLO(best_phase1)\n",
    "print(\"üìä Evaluando Fase 1 (val)...\")\n",
    "val1 = model_eval1.val(data=FINAL_DATA_YAML, split='val')  # dict metrics\n",
    "m1 = {\n",
    "  'mAP50': float(val1.results_dict.get('metrics/mAP50(B)',0)),\n",
    "  'mAP50_95': float(val1.results_dict.get('metrics/mAP50-95(B)',0)),\n",
    "  'precision': float(val1.results_dict.get('metrics/precision(B)',0)),\n",
    "  'recall': float(val1.results_dict.get('metrics/recall(B)',0))\n",
    "}\n",
    "print(json.dumps(m1, indent=2))\n",
    "with open(os.path.join(PHASE1_DIR,'evaluation_phase1.json'),'w') as f: json.dump(m1,f,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================\n",
    "# 6. Fase 2 - Fine-Tune de Refinamiento\n",
    "# =============================================\n",
    "phase2_name = f\"{phase1_name}_finetune\"\n",
    "PHASE2_DIR = os.path.join(RESULTS_ROOT, phase2_name)\n",
    "\n",
    "phase2_args = dict(\n",
    "    epochs=PHASE2_EPOCHS,\n",
    "    patience=PHASE2_PATIENCE,\n",
    "    imgsz=IMG_SIZE_PHASE2,\n",
    "    batch=max(8, BASE_BATCH-4),\n",
    "    workers=4,\n",
    "    device=0 if torch.cuda.is_available() else 'cpu',\n",
    "    amp=True,\n",
    "    cache=False,\n",
    "    lr0=0.003, lrf=0.01, momentum=0.937, weight_decay=0.0008,\n",
    "    warmup_epochs=2,\n",
    "    label_smoothing=0.05,\n",
    "    hsv_h=0.005, hsv_s=0.20, hsv_v=0.15,\n",
    "    degrees=3, translate=0.03, scale=0.12, shear=0.5,\n",
    "    perspective=0.0, flipud=0.0, fliplr=0.5,\n",
    "    mosaic=0.0, mixup=0.05, copy_paste=0.05, close_mosaic=0,\n",
    "    box=7.5, cls=1.0, dfl=1.5,\n",
    "    optimizer='AdamW',\n",
    "    project=RESULTS_ROOT, name=phase2_name, exist_ok=True,\n",
    "    save=True, save_period=5, plots=True, verbose=True\n",
    ")\n",
    "print(\"‚öôÔ∏è Config Fase 2 (resumen):\")\n",
    "print(json.dumps({k: phase2_args[k] for k in ['epochs','imgsz','batch','lr0','mosaic','mixup','cls']}, indent=2))\n",
    "\n",
    "print(f\"üì• Cargando pesos Fase1: {best_phase1}\")\n",
    "model_phase2 = YOLO(best_phase1)\n",
    "t0 = time.time(); print(\"üöÄ Entrenando Fase 2...\")\n",
    "res_phase2 = model_phase2.train(data=FINAL_DATA_YAML, **phase2_args)\n",
    "t2 = (time.time()-t0)/60\n",
    "print(f\"‚úÖ Fase 2 completada en {t2:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================\n",
    "# 7. Evaluaci√≥n Fase 2\n",
    "# =============================================\n",
    "best_phase2 = os.path.join(PHASE2_DIR,'weights','best.pt')\n",
    "assert os.path.exists(best_phase2), 'best.pt Fase2 no encontrado'\n",
    "model_eval2 = YOLO(best_phase2)\n",
    "print(\"üìä Evaluando Fase 2 (val)...\")\n",
    "val2 = model_eval2.val(data=FINAL_DATA_YAML, split='val')\n",
    "m2 = {\n",
    "  'mAP50': float(val2.results_dict.get('metrics/mAP50(B)',0)),\n",
    "  'mAP50_95': float(val2.results_dict.get('metrics/mAP50-95(B)',0)),\n",
    "  'precision': float(val2.results_dict.get('metrics/precision(B)',0)),\n",
    "  'recall': float(val2.results_dict.get('metrics/recall(B)',0))\n",
    "}\n",
    "print(json.dumps(m2, indent=2))\n",
    "with open(os.path.join(PHASE2_DIR,'evaluation_phase2.json'),'w') as f: json.dump(m2,f,indent=2)\n",
    "\n",
    "# Consolidado\n",
    "consolidated = {'phase1': m1, 'phase2': m2}\n",
    "with open(os.path.join(PHASE2_DIR,'evaluation_consolidated.json'),'w') as f: json.dump(consolidated,f,indent=2)\n",
    "print(\"üíæ evaluation_consolidated.json creado\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================\n",
    "# 8. Visualizaciones Comparativas\n",
    "# =============================================\n",
    "def load_results_csv(d):\n",
    "    p = os.path.join(d,'results.csv')\n",
    "    return pd.read_csv(p) if os.path.exists(p) else None\n",
    "\n",
    "df1 = load_results_csv(PHASE1_DIR)\n",
    "df2 = load_results_csv(PHASE2_DIR)\n",
    "assert df1 is not None and df2 is not None, 'results.csv faltante'\n",
    "\n",
    "viz_dir = os.path.join(PHASE2_DIR,'visualizations')\n",
    "os.makedirs(viz_dir, exist_ok=True)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(df1['metrics/mAP50(B)'], label='Phase1 mAP50', c='royalblue')\n",
    "plt.plot(range(len(df1), len(df1)+len(df2)), df2['metrics/mAP50(B)'], label='Phase2 mAP50', c='navy')\n",
    "plt.title('Evoluci√≥n mAP@0.5'); plt.xlabel('√âpoca global'); plt.ylabel('mAP50'); plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(df1['metrics/precision(B)'], label='Phase1 Precision', c='green')\n",
    "plt.plot(range(len(df1), len(df1)+len(df2)), df2['metrics/precision(B)'], label='Phase2 Precision', c='darkgreen')\n",
    "plt.title('Evoluci√≥n Precision'); plt.xlabel('√âpoca global'); plt.ylabel('Precision'); plt.legend()\n",
    "p_curve = os.path.join(viz_dir,'phase_comparison_curves.png')\n",
    "plt.tight_layout(); plt.savefig(p_curve, dpi=220); plt.show()\n",
    "\n",
    "labels = ['mAP50','Precision','Recall']\n",
    "phase1_vals = [m1['mAP50'], m1['precision'], m1['recall']]\n",
    "phase2_vals = [m2['mAP50'], m2['precision'], m2['recall']]\n",
    "targets = [0.75,0.75,0.75]\n",
    "x = np.arange(len(labels)); w = 0.25\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(x-w, phase1_vals, w, label='Phase1', color='#87b5ff')\n",
    "plt.bar(x, phase2_vals, w, label='Phase2', color='#7ae29a')\n",
    "plt.bar(x+w, targets, w, label='Objetivo', color='#f2d46b')\n",
    "for i,v in enumerate(phase2_vals): plt.text(i, v+0.01, f\"{v:.3f}\", ha='center')\n",
    "plt.xticks(x, labels); plt.ylabel('Score'); plt.title('Comparaci√≥n Fase1 vs Fase2 vs Objetivo'); plt.ylim(0, max(1.0,max(phase2_vals)+0.1)); plt.legend()\n",
    "p_bar = os.path.join(viz_dir,'phase_comparison_bars.png')\n",
    "plt.savefig(p_bar, dpi=220, bbox_inches='tight'); plt.show()\n",
    "\n",
    "with open(os.path.join(viz_dir,'visualizations_index.json'),'w') as f:\n",
    "    json.dump({'curves': p_curve, 'bars': p_bar, 'phase1': m1, 'phase2': m2}, f, indent=2)\n",
    "print('‚úÖ Visualizaciones guardadas')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================\n",
    "# 9. Exportaciones (para despliegue)\n",
    "# =============================================\n",
    "print('üì¶ Exportando modelo final (ONNX, TorchScript)...')\n",
    "final_model = YOLO(best_phase2 if os.path.exists(best_phase2) else best_phase1)\n",
    "EXPORT_DIR = os.path.join(PHASE2_DIR,'exports'); os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "os.chdir(EXPORT_DIR)\n",
    "final_model.export(format='onnx', opset=12, simplify=True)\n",
    "final_model.export(format='torchscript')\n",
    "print('‚úÖ Export completado: ONNX y TorchScript')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Finalizaci√≥n\n",
    "**Directorios clave:**  \n",
    "- Fase 1: `", 
    "PHASE1_DIR",
    "`  \n",
    "- Fase 2: `", 
    "PHASE2_DIR",
    "`  \n",
    "Archivos relevantes: best.pt (fase2), evaluation_phase2.json, visualizations, exports.\n",
    "\n",
    "**Siguiente:** Integrar best.pt en pipeline de inferencia y documentar mejoras fase2."
   ]
  }
 ]
}